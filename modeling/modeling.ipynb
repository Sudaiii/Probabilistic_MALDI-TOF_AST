{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/castudil/bacteria-multi-label/blob/main/multilabel_bac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKUeIuZcpHUl"
   },
   "source": [
    "Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bzVprbfpWSLa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import (f1_score, multilabel_confusion_matrix,\n",
    "                             accuracy_score, hamming_loss, jaccard_score, make_scorer)\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "nQsWnulDWdDD",
    "outputId": "6f97687c-b560-4e34-fd8b-9c3ef7aeb0c7",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.016147</td>\n",
       "      <td>0.016983</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.019405</td>\n",
       "      <td>0.023356</td>\n",
       "      <td>0.026224</td>\n",
       "      <td>0.026569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037966</td>\n",
       "      <td>0.030364</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>0.040851</td>\n",
       "      <td>0.034176</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.025638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009001</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014496</td>\n",
       "      <td>0.024966</td>\n",
       "      <td>0.027437</td>\n",
       "      <td>0.026541</td>\n",
       "      <td>0.022940</td>\n",
       "      <td>0.020572</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>0.020910</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.021436</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.020229</td>\n",
       "      <td>0.018818</td>\n",
       "      <td>0.018637</td>\n",
       "      <td>0.018815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024620</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.026715</td>\n",
       "      <td>0.032045</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>0.029085</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.016407</td>\n",
       "      <td>0.018011</td>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.019116</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051312</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.049338</td>\n",
       "      <td>0.055039</td>\n",
       "      <td>0.054541</td>\n",
       "      <td>0.058643</td>\n",
       "      <td>0.058919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236769</td>\n",
       "      <td>0.217499</td>\n",
       "      <td>0.187244</td>\n",
       "      <td>0.216243</td>\n",
       "      <td>0.221910</td>\n",
       "      <td>0.226531</td>\n",
       "      <td>0.221965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>0.056616</td>\n",
       "      <td>0.039011</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>0.048517</td>\n",
       "      <td>0.050865</td>\n",
       "      <td>0.047771</td>\n",
       "      <td>0.049312</td>\n",
       "      <td>0.048257</td>\n",
       "      <td>0.049417</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021169</td>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.021037</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>0.125837</td>\n",
       "      <td>0.107712</td>\n",
       "      <td>0.109186</td>\n",
       "      <td>0.107613</td>\n",
       "      <td>0.109855</td>\n",
       "      <td>0.105060</td>\n",
       "      <td>0.099640</td>\n",
       "      <td>0.104169</td>\n",
       "      <td>0.120303</td>\n",
       "      <td>0.125067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082375</td>\n",
       "      <td>0.083446</td>\n",
       "      <td>0.096510</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.092228</td>\n",
       "      <td>0.085599</td>\n",
       "      <td>0.042142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035603</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>0.042372</td>\n",
       "      <td>0.046666</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.043914</td>\n",
       "      <td>0.039875</td>\n",
       "      <td>0.037170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.006124</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049241</td>\n",
       "      <td>0.039586</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.039139</td>\n",
       "      <td>0.046816</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.022810</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>0.026694</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.025140</td>\n",
       "      <td>0.026609</td>\n",
       "      <td>0.024203</td>\n",
       "      <td>0.020953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090198</td>\n",
       "      <td>0.101889</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>0.122065</td>\n",
       "      <td>0.095740</td>\n",
       "      <td>0.082289</td>\n",
       "      <td>0.081714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2824 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2000      2001      2002      2003      2004      2005      2006   \n",
       "0     0.018721  0.016147  0.016983  0.021218  0.020846  0.019784  0.019405  \\\n",
       "1     0.009001  0.007475  0.006874  0.008575  0.009539  0.007894  0.008314   \n",
       "2     0.022354  0.020220  0.020910  0.024631  0.021436  0.021197  0.020229   \n",
       "3     0.017619  0.016073  0.016407  0.018011  0.019364  0.018950  0.017607   \n",
       "4     0.008264  0.008229  0.006753  0.006657  0.010107  0.007039  0.008250   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2819  0.056616  0.039011  0.040380  0.048517  0.050865  0.047771  0.049312   \n",
       "2820  0.125837  0.107712  0.109186  0.107613  0.109855  0.105060  0.099640   \n",
       "2821  0.000000  0.000000  0.035603  0.039994  0.042372  0.046666  0.045781   \n",
       "2822  0.005443  0.005998  0.003670  0.005588  0.006124  0.005019  0.004853   \n",
       "2823  0.026184  0.022810  0.023200  0.020464  0.026694  0.024624  0.025140   \n",
       "\n",
       "          2007      2008      2009  ...      9993      9994      9995   \n",
       "0     0.023356  0.026224  0.026569  ...  0.037966  0.030364  0.037545  \\\n",
       "1     0.008013  0.008664  0.008923  ...  0.014496  0.024966  0.027437   \n",
       "2     0.018818  0.018637  0.018815  ...  0.024620  0.022942  0.026715   \n",
       "3     0.019116  0.023623  0.024492  ...  0.051312  0.047458  0.049338   \n",
       "4     0.010670  0.008134  0.006513  ...  0.236769  0.217499  0.187244   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2819  0.048257  0.049417  0.049000  ...  0.021169  0.023617  0.033694   \n",
       "2820  0.104169  0.120303  0.125067  ...  0.082375  0.083446  0.096510   \n",
       "2821  0.043914  0.039875  0.037170  ...  0.006903  0.008322  0.011071   \n",
       "2822  0.005400  0.004169  0.005151  ...  0.049241  0.039586  0.050542   \n",
       "2823  0.026609  0.024203  0.020953  ...  0.090198  0.101889  0.105174   \n",
       "\n",
       "          9996      9997      9998      9999  Oxacillin  Clindamycin   \n",
       "0     0.040851  0.034176  0.046110  0.025638        0.0          0.0  \\\n",
       "1     0.026541  0.022940  0.020572  0.032504        0.0          0.0   \n",
       "2     0.032045  0.030431  0.029085  0.013117        0.0          0.0   \n",
       "3     0.055039  0.054541  0.058643  0.058919        0.0          0.0   \n",
       "4     0.216243  0.221910  0.226531  0.221965        0.0          0.0   \n",
       "...        ...       ...       ...       ...        ...          ...   \n",
       "2819  0.021037  0.018727  0.010641  0.009238        0.0          1.0   \n",
       "2820  0.084883  0.092228  0.085599  0.042142        0.0          1.0   \n",
       "2821  0.010274  0.004682  0.003547  0.001744        0.0          0.0   \n",
       "2822  0.039139  0.046816  0.043036  0.037402        1.0          1.0   \n",
       "2823  0.122065  0.095740  0.082289  0.081714        0.0          0.0   \n",
       "\n",
       "      Fusidic acid  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  \n",
       "...            ...  \n",
       "2819           0.0  \n",
       "2820           0.0  \n",
       "2821           0.0  \n",
       "2822           0.0  \n",
       "2823           0.0  \n",
       "\n",
       "[2824 rows x 8003 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = \"data/processed/raw/train_s_aureus_driams.csv\"\n",
    "train_bac = pd.read_csv(train_file)\n",
    "train_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044453</td>\n",
       "      <td>0.032486</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>0.034223</td>\n",
       "      <td>0.037528</td>\n",
       "      <td>0.039503</td>\n",
       "      <td>0.031378</td>\n",
       "      <td>0.035506</td>\n",
       "      <td>0.037688</td>\n",
       "      <td>0.035658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247814</td>\n",
       "      <td>0.263833</td>\n",
       "      <td>0.279904</td>\n",
       "      <td>0.264432</td>\n",
       "      <td>0.241573</td>\n",
       "      <td>0.266020</td>\n",
       "      <td>0.231517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004318</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033364</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>0.066426</td>\n",
       "      <td>0.057485</td>\n",
       "      <td>0.052903</td>\n",
       "      <td>0.050839</td>\n",
       "      <td>0.036631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026459</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.028609</td>\n",
       "      <td>0.031314</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.028051</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>0.028978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.094103</td>\n",
       "      <td>0.080969</td>\n",
       "      <td>0.073970</td>\n",
       "      <td>0.069047</td>\n",
       "      <td>0.070988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>0.014582</td>\n",
       "      <td>0.015084</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.018272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.007583</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060499</td>\n",
       "      <td>0.043034</td>\n",
       "      <td>0.030296</td>\n",
       "      <td>0.032635</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>0.032762</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>0.030382</td>\n",
       "      <td>0.030233</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029452</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>0.039230</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>0.037219</td>\n",
       "      <td>0.022227</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.013092</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.009902</td>\n",
       "      <td>0.013513</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.010994</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050851</td>\n",
       "      <td>0.046334</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.066948</td>\n",
       "      <td>0.061007</td>\n",
       "      <td>0.056178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.021222</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>0.015512</td>\n",
       "      <td>0.017995</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.019427</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047285</td>\n",
       "      <td>0.056455</td>\n",
       "      <td>0.058002</td>\n",
       "      <td>0.055528</td>\n",
       "      <td>0.043539</td>\n",
       "      <td>0.039962</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>0.045613</td>\n",
       "      <td>0.041040</td>\n",
       "      <td>0.046177</td>\n",
       "      <td>0.050216</td>\n",
       "      <td>0.046525</td>\n",
       "      <td>0.048843</td>\n",
       "      <td>0.045920</td>\n",
       "      <td>0.044283</td>\n",
       "      <td>0.043983</td>\n",
       "      <td>0.046330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104234</td>\n",
       "      <td>0.094692</td>\n",
       "      <td>0.090012</td>\n",
       "      <td>0.096624</td>\n",
       "      <td>0.092228</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.043682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.015193</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>0.010474</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.014157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051772</td>\n",
       "      <td>0.058255</td>\n",
       "      <td>0.074609</td>\n",
       "      <td>0.068249</td>\n",
       "      <td>0.049157</td>\n",
       "      <td>0.070229</td>\n",
       "      <td>0.043615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.022209</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.025181</td>\n",
       "      <td>0.025069</td>\n",
       "      <td>0.025761</td>\n",
       "      <td>0.022690</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052692</td>\n",
       "      <td>0.061853</td>\n",
       "      <td>0.048375</td>\n",
       "      <td>0.060665</td>\n",
       "      <td>0.043305</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>0.048284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>707 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2000      2001      2002      2003      2004      2005      2006   \n",
       "0    0.044453  0.032486  0.032540  0.034223  0.037528  0.039503  0.031378  \\\n",
       "1    0.004318  0.001881  0.001274  0.000902  0.000892  0.000049  0.002188   \n",
       "2    0.026184  0.026459  0.025393  0.028609  0.031314  0.031739  0.033337   \n",
       "3    0.000000  0.015010  0.017782  0.014582  0.015084  0.018046  0.014461   \n",
       "4    0.060499  0.043034  0.030296  0.032635  0.031272  0.032762  0.031154   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "702  0.013092  0.011730  0.009902  0.013513  0.015317  0.011370  0.013338   \n",
       "703  0.021222  0.015896  0.015512  0.017995  0.018663  0.019427  0.018667   \n",
       "704  0.045613  0.041040  0.046177  0.050216  0.046525  0.048843  0.045920   \n",
       "705  0.015193  0.011922  0.010877  0.009975  0.010474  0.012171  0.010417   \n",
       "706  0.025453  0.022209  0.022442  0.025181  0.025069  0.025761  0.022690   \n",
       "\n",
       "         2007      2008      2009  ...      9993      9994      9995   \n",
       "0    0.035506  0.037688  0.035658  ...  0.247814  0.263833  0.279904  \\\n",
       "1    0.002001  0.003081  0.003384  ...  0.033364  0.042735  0.066426   \n",
       "2    0.028051  0.028047  0.028978  ...  0.086746  0.087719  0.094103   \n",
       "3    0.014400  0.018769  0.018272  ...  0.005062  0.006748  0.004573   \n",
       "4    0.030382  0.030233  0.029438  ...  0.029452  0.033288  0.039230   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "702  0.010117  0.010994  0.009222  ...  0.050851  0.046334  0.042840   \n",
       "703  0.014589  0.016765  0.015071  ...  0.047285  0.056455  0.058002   \n",
       "704  0.044283  0.043983  0.046330  ...  0.104234  0.094692  0.090012   \n",
       "705  0.010783  0.013170  0.014157  ...  0.051772  0.058255  0.074609   \n",
       "706  0.024250  0.022596  0.025572  ...  0.052692  0.061853  0.048375   \n",
       "\n",
       "         9996      9997      9998      9999  Oxacillin  Clindamycin   \n",
       "0    0.264432  0.241573  0.266020  0.231517        0.0          0.0  \\\n",
       "1    0.057485  0.052903  0.050839  0.036631        0.0          0.0   \n",
       "2    0.080969  0.073970  0.069047  0.070988        0.0          1.0   \n",
       "3    0.007583  0.008427  0.004729  0.002394        0.0          0.0   \n",
       "4    0.046477  0.037219  0.022227  0.019920        0.0          0.0   \n",
       "..        ...       ...       ...       ...        ...          ...   \n",
       "702  0.061644  0.066948  0.061007  0.056178        0.0          0.0   \n",
       "703  0.055528  0.043539  0.039962  0.034761        1.0          0.0   \n",
       "704  0.096624  0.092228  0.100024  0.043682        0.0          0.0   \n",
       "705  0.068249  0.049157  0.070229  0.043615        1.0          0.0   \n",
       "706  0.060665  0.043305  0.041617  0.048284        0.0          0.0   \n",
       "\n",
       "     Fusidic acid  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             1.0  \n",
       "4             0.0  \n",
       "..            ...  \n",
       "702           0.0  \n",
       "703           0.0  \n",
       "704           1.0  \n",
       "705           0.0  \n",
       "706           0.0  \n",
       "\n",
       "[707 rows x 8003 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = \"data/processed/raw/test_s_aureus_driams.csv\"\n",
    "test_bac = pd.read_csv(test_file)\n",
    "test_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rXEshQwKQuzR",
    "outputId": "32756288-dd35-49f1-be9b-34bfe433baf7",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_bac[train_bac.columns.drop(list(train_bac.filter(regex='[^0-9]')))]\n",
    "test_x = test_bac[test_bac.columns.drop(list(test_bac.filter(regex='[^0-9]')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GiCCaGOEQuzS",
    "outputId": "bdc15429-4a19-4332-d59f-dd1c0ce37d88",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "antibiotics = train_bac.columns.drop(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_bac[antibiotics]\n",
    "test_y = test_bac[antibiotics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multilabel_f1_wrapper(true, pred, average=\"weighted\"):\n",
    "    if isinstance(true, list):\n",
    "        true = np.array(true)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        true = true.to_numpy()\n",
    "    if isinstance(pred, list):\n",
    "        pred = np.array(pred)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        pred = pred.to_numpy()\n",
    "    column = 0\n",
    "    total = 0\n",
    "    while column < true[0].size:\n",
    "        total+=f1_score(true[:, column], pred[:, column], average=average)\n",
    "        column+=1\n",
    "    return total/(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def report(true, pred):\n",
    "        \n",
    "    hl = hamming_loss(true, pred)\n",
    "    f1w = multilabel_f1_wrapper(true, pred, \"weighted\")\n",
    "    acc = accuracy_score(true, pred)\n",
    "    \n",
    "    f1u = multilabel_f1_wrapper(true, pred, \"macro\")\n",
    "    f1su = f1_score(true, pred, average=\"macro\")\n",
    "    f1sw = f1_score(true, pred, average=\"weighted\")\n",
    "\n",
    "    \n",
    "    print(\"Main metrics:\")\n",
    "    print(\" Hamming Loss:\", hl)\n",
    "    print(\" Accuracy:\", acc)\n",
    "    print(\" F1 Score (Weighted):\", f1w)\n",
    "    print(\"================================================\")\n",
    "    print(\"Other metrics:\")\n",
    "    print(\" F1 Score (Unweighted):\", f1u)\n",
    "    print(\" F1 Score (sklearn Unweighted):\", f1su)\n",
    "    print(\" F1 Score (sklearn Weighted):\", f1sw)\n",
    "    return hl, acc, f1w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-gP_gv8QuzZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Development\\Projects\\Probabilistic_MALDI-TOF_AST\\modeling\\modeling_svm.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/d%3A/Development/Projects/Probabilistic_MALDI-TOF_AST/modeling/modeling_svm.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bayesopt \u001b[39m=\u001b[39m BayesSearchCV(\n\u001b[1;32m      <a href='vscode-notebook-cell:/d%3A/Development/Projects/Probabilistic_MALDI-TOF_AST/modeling/modeling_svm.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ClassifierChain(xgb\u001b[39m.\u001b[39mXGBClassifier(), random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/d%3A/Development/Projects/Probabilistic_MALDI-TOF_AST/modeling/modeling_svm.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/d%3A/Development/Projects/Probabilistic_MALDI-TOF_AST/modeling/modeling_svm.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/d%3A/Development/Projects/Probabilistic_MALDI-TOF_AST/modeling/modeling_svm.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/d%3A/Development/Projects/Probabilistic_MALDI-TOF_AST/modeling/modeling_svm.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m bayesopt\u001b[39m.\u001b[39;49mfit(train_x, train_y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/d%3A/Development/Projects/Probabilistic_MALDI-TOF_AST/modeling/modeling_svm.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest score:\u001b[39m\u001b[39m\"\u001b[39m, bayesopt\u001b[39m.\u001b[39mbest_score_)\n\u001b[1;32m     <a href='vscode-notebook-cell:/d%3A/Development/Projects/Probabilistic_MALDI-TOF_AST/modeling/modeling_svm.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest parameter combination found:\u001b[39m\u001b[39m\"\u001b[39m, bayesopt\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/bac/venv/lib/python3.9/site-packages/skopt/searchcv.py:466\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_kwargs_ \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_kwargs)\n\u001b[0;32m--> 466\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, groups\u001b[39m=\u001b[39;49mgroups, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    468\u001b[0m \u001b[39m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m~/bac/venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/bac/venv/lib/python3.9/site-packages/skopt/searchcv.py:512\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39mwhile\u001b[39;00m n_iter \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     n_points_adjusted \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_iter, n_points)\n\u001b[0;32m--> 512\u001b[0m     optim_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(\n\u001b[1;32m    513\u001b[0m         search_space, optimizer,\n\u001b[1;32m    514\u001b[0m         evaluate_candidates, n_points\u001b[39m=\u001b[39;49mn_points_adjusted\n\u001b[1;32m    515\u001b[0m     )\n\u001b[1;32m    516\u001b[0m     n_iter \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n_points\n\u001b[1;32m    518\u001b[0m     \u001b[39mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[0;32m~/bac/venv/lib/python3.9/site-packages/skopt/searchcv.py:408\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[0;34m(self, search_space, optimizer, evaluate_candidates, n_points)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m# make lists into dictionaries\u001b[39;00m\n\u001b[1;32m    406\u001b[0m params_dict \u001b[39m=\u001b[39m [point_asdict(search_space, p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params]\n\u001b[0;32m--> 408\u001b[0m all_results \u001b[39m=\u001b[39m evaluate_candidates(params_dict)\n\u001b[1;32m    409\u001b[0m \u001b[39m# Feed the point and objective value back into optimizer\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m# Optimizer minimizes objective, hence provide negative score\u001b[39;00m\n\u001b[1;32m    411\u001b[0m local_results \u001b[39m=\u001b[39m all_results[\u001b[39m\"\u001b[39m\u001b[39mmean_test_score\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(params):]\n",
      "File \u001b[0;32m~/bac/venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/bac/venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/bac/venv/lib/python3.9/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/bac/venv/lib/python3.9/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/bac/venv/lib/python3.9/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:435\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 435\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bayesopt = BayesSearchCV(\n",
    "    ClassifierChain(xgb.XGBClassifier(), random_state=0),\n",
    "    {\n",
    "        \"base_estimator__objective\": Categorical([\"binary:logistic\"]),\n",
    "        \"base_estimator__max_depth\": Integer(1, 10),\n",
    "        \"base_estimator__min_child_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__max_delta_step\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__subsample\": Real(1e-6, 1, prior=\"log-uniform\"),\n",
    "        \"base_estimator__tree_method\": Categorical([\"exact\", \"approx\", \"hist\"]),\n",
    "        \"base_estimator__scale_pos_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__gamma\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__eta\": Real(1e-6, 1, prior=\"log-uniform\")\n",
    "    },\n",
    "    n_iter=600,\n",
    "    cv=5,\n",
    "    random_state=0,\n",
    "    n_jobs=5,\n",
    "    n_points=2,\n",
    "    scoring=make_scorer(multilabel_f1_wrapper),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "bayesopt.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best score:\", bayesopt.best_score_)\n",
    "print(\"Best parameter combination found:\", bayesopt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the model constructed by the parameter optimizer to predict the test data see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main metrics:\n",
      " Hamming Loss: 0.09824046920821114\n",
      " Accuracy: 0.7565982404692082\n",
      " Jaccard Score (Weighted): 0.8242632743158126\n",
      " F1 Score (Weighted): 0.8850588072459757\n",
      "================================================\n",
      "Other metrics:\n",
      " Jaccard Score (Unweighted): 0.5983736545267067\n",
      " Jaccard Score (sklearn Unweighted): 0.30167052991070015\n",
      " Jaccard Score (sklearn Weighted): 0.5157815364711917\n",
      " F1 Score (Unweighted): 0.6853390461031316\n",
      " F1 Score (sklearn Unweighted): 0.4262847412272699\n",
      " F1 Score (sklearn Weighted): 0.5157815364711917\n"
     ]
    }
   ],
   "source": [
    "model = bayesopt.best_estimator_\n",
    "model.fit(train_x, train_y) \n",
    "pred = model.predict(test_x)\n",
    "model_hl, model_acc, model_js, model_f1 = report(test_y, (pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, 'xgb_s_aureus_raw.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further visualize the results through a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, len(antibiotics), figsize=(len(antibiotics)*5, 5))\n",
    "fig.supxlabel(\"Predicted Label\")\n",
    "fig.supylabel(\"True Label\")\n",
    "\n",
    "cm_svm_c = multilabel_confusion_matrix(test_y, pred)\n",
    "\n",
    "for i in range(len(antibiotics)):\n",
    "  sns.heatmap(ax=axes[i], data=cm_svm_c[i], annot=True, fmt='d', cbar=None, cmap=\"Blues\", xticklabels=[\"S\", \"R\"], yticklabels=[\"S\", \"R\"]).set(title=antibiotics[i])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
