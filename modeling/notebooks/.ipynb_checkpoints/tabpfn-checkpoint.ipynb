{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/castudil/bacteria-multi-label/blob/main/multilabel_bac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKUeIuZcpHUl"
   },
   "source": [
    "Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bzVprbfpWSLa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n",
    "from sklearn.metrics import (f1_score, multilabel_confusion_matrix,\n",
    "                             accuracy_score, hamming_loss, jaccard_score, make_scorer)\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bac = \"s_aureus_driams\"\n",
    "input_folder = \"data/processed/raw/\"\n",
    "normalization = \"none/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "nQsWnulDWdDD",
    "outputId": "6f97687c-b560-4e34-fd8b-9c3ef7aeb0c7",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>537.666667</td>\n",
       "      <td>546.500000</td>\n",
       "      <td>571.000000</td>\n",
       "      <td>679.0</td>\n",
       "      <td>660.500000</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>727.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>165.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>45.101586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258.500000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>237.666667</td>\n",
       "      <td>277.0</td>\n",
       "      <td>305.666667</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>267.500000</td>\n",
       "      <td>255.333333</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>63.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>108.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>57.160932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642.000000</td>\n",
       "      <td>684.333333</td>\n",
       "      <td>700.500000</td>\n",
       "      <td>787.5</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>665.500000</td>\n",
       "      <td>638.666667</td>\n",
       "      <td>587.500000</td>\n",
       "      <td>571.000000</td>\n",
       "      <td>566.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>107.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>23.112694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>544.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>577.0</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>596.333333</td>\n",
       "      <td>557.000000</td>\n",
       "      <td>596.666667</td>\n",
       "      <td>721.500000</td>\n",
       "      <td>737.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>223.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>103.553762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>237.333333</td>\n",
       "      <td>278.500000</td>\n",
       "      <td>233.666667</td>\n",
       "      <td>216.0</td>\n",
       "      <td>323.500000</td>\n",
       "      <td>229.666667</td>\n",
       "      <td>265.500000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>778.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>389.911454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>1626.000000</td>\n",
       "      <td>1320.333333</td>\n",
       "      <td>1342.500000</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>1602.500000</td>\n",
       "      <td>1483.500000</td>\n",
       "      <td>1544.666667</td>\n",
       "      <td>1492.500000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1475.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.298639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>3614.000000</td>\n",
       "      <td>3645.500000</td>\n",
       "      <td>3611.333333</td>\n",
       "      <td>3426.0</td>\n",
       "      <td>3453.666667</td>\n",
       "      <td>3247.000000</td>\n",
       "      <td>3112.500000</td>\n",
       "      <td>3211.333333</td>\n",
       "      <td>3639.500000</td>\n",
       "      <td>3764.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>358.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>74.088595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1185.000000</td>\n",
       "      <td>1276.0</td>\n",
       "      <td>1336.000000</td>\n",
       "      <td>1449.500000</td>\n",
       "      <td>1434.666667</td>\n",
       "      <td>1359.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1119.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.136758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>156.333333</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>182.0</td>\n",
       "      <td>198.500000</td>\n",
       "      <td>167.500000</td>\n",
       "      <td>159.666667</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>134.333333</td>\n",
       "      <td>155.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>214.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>65.763190</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>752.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>776.000000</td>\n",
       "      <td>655.0</td>\n",
       "      <td>844.000000</td>\n",
       "      <td>771.000000</td>\n",
       "      <td>791.666667</td>\n",
       "      <td>827.000000</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>631.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>392.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>143.588679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2824 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             2000         2001         2002    2003         2004         2005   \n",
       "0      537.666667   546.500000   571.000000   679.0   660.500000   622.000000  \\\n",
       "1      258.500000   253.000000   237.666667   277.0   305.666667   256.000000   \n",
       "2      642.000000   684.333333   700.500000   787.5   679.000000   665.500000   \n",
       "3      506.000000   544.000000   552.000000   577.0   614.000000   596.333333   \n",
       "4      237.333333   278.500000   233.666667   216.0   323.500000   229.666667   \n",
       "...           ...          ...          ...     ...          ...          ...   \n",
       "2819  1626.000000  1320.333333  1342.500000  1547.0  1602.500000  1483.500000   \n",
       "2820  3614.000000  3645.500000  3611.333333  3426.0  3453.666667  3247.000000   \n",
       "2821     0.000000     0.000000  1185.000000  1276.0  1336.000000  1449.500000   \n",
       "2822   156.333333   203.000000   132.000000   182.0   198.500000   167.500000   \n",
       "2823   752.000000   772.000000   776.000000   655.0   844.000000   771.000000   \n",
       "\n",
       "             2006         2007         2008         2009  ...    9993   9994   \n",
       "0      613.000000   727.000000   800.000000   800.000000  ...   165.0  135.0  \\\n",
       "1      267.500000   255.333333   270.000000   269.000000  ...    63.0  111.0   \n",
       "2      638.666667   587.500000   571.000000   566.666667  ...   107.0  102.0   \n",
       "3      557.000000   596.666667   721.500000   737.500000  ...   223.0  211.0   \n",
       "4      265.500000   337.000000   254.000000   196.500000  ...  1029.0  967.0   \n",
       "...           ...          ...          ...          ...  ...     ...    ...   \n",
       "2819  1544.666667  1492.500000  1500.000000  1475.000000  ...    92.0  105.0   \n",
       "2820  3112.500000  3211.333333  3639.500000  3764.000000  ...   358.0  371.0   \n",
       "2821  1434.666667  1359.000000  1212.000000  1119.000000  ...    30.0   37.0   \n",
       "2822   159.666667   175.000000   134.333333   155.500000  ...   214.0  176.0   \n",
       "2823   791.666667   827.000000   739.000000   631.000000  ...   392.0  453.0   \n",
       "\n",
       "       9995   9996   9997   9998        9999  Oxacillin  Clindamycin   \n",
       "0     156.0  167.0  146.0  195.0   45.101586        0.0          0.0  \\\n",
       "1     114.0  108.5   98.0   87.0   57.160932        0.0          0.0   \n",
       "2     111.0  131.0  130.0  123.0   23.112694        0.0          0.0   \n",
       "3     205.0  225.0  233.0  248.0  103.553762        0.0          0.0   \n",
       "4     778.0  884.0  948.0  958.0  389.911454        0.0          0.0   \n",
       "...     ...    ...    ...    ...         ...        ...          ...   \n",
       "2819  140.0   86.0   80.0   45.0   16.298639        0.0          1.0   \n",
       "2820  401.0  347.0  394.0  362.0   74.088595        0.0          1.0   \n",
       "2821   46.0   42.0   20.0   15.0    3.136758        0.0          0.0   \n",
       "2822  210.0  160.0  200.0  182.0   65.763190        1.0          1.0   \n",
       "2823  437.0  499.0  409.0  348.0  143.588679        0.0          0.0   \n",
       "\n",
       "      Fusidic acid  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  \n",
       "...            ...  \n",
       "2819           0.0  \n",
       "2820           0.0  \n",
       "2821           0.0  \n",
       "2822           0.0  \n",
       "2823           0.0  \n",
       "\n",
       "[2824 rows x 8003 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = input_folder+normalization+\"train_\"+bac+\".csv\"\n",
    "train_bac = pd.read_csv(train_file)\n",
    "train_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1276.666667</td>\n",
       "      <td>1099.500000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>1092.5</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1229.000000</td>\n",
       "      <td>986.000000</td>\n",
       "      <td>1100.500000</td>\n",
       "      <td>1146.000000</td>\n",
       "      <td>1073.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>406.687788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124.000000</td>\n",
       "      <td>63.666667</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.333333</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>76.666667</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>101.500000</td>\n",
       "      <td>102.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>145.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>64.410202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>752.000000</td>\n",
       "      <td>895.500000</td>\n",
       "      <td>848.333333</td>\n",
       "      <td>914.0</td>\n",
       "      <td>989.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>1047.000000</td>\n",
       "      <td>871.333333</td>\n",
       "      <td>855.000000</td>\n",
       "      <td>872.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>377.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>124.750563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>597.333333</td>\n",
       "      <td>468.0</td>\n",
       "      <td>479.666667</td>\n",
       "      <td>568.500000</td>\n",
       "      <td>459.000000</td>\n",
       "      <td>451.666667</td>\n",
       "      <td>575.000000</td>\n",
       "      <td>550.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.279255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1737.500000</td>\n",
       "      <td>1456.500000</td>\n",
       "      <td>1010.000000</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>987.666667</td>\n",
       "      <td>1021.500000</td>\n",
       "      <td>979.000000</td>\n",
       "      <td>943.000000</td>\n",
       "      <td>921.000000</td>\n",
       "      <td>886.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>128.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>35.059556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>376.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>337.500000</td>\n",
       "      <td>434.0</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>340.333333</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>221.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>98.740466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>609.500000</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>522.500000</td>\n",
       "      <td>576.5</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>611.000000</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>457.500000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>454.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>205.5</td>\n",
       "      <td>251.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>61.124341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>1310.000000</td>\n",
       "      <td>1389.000000</td>\n",
       "      <td>1533.666667</td>\n",
       "      <td>1601.0</td>\n",
       "      <td>1466.333333</td>\n",
       "      <td>1516.500000</td>\n",
       "      <td>1439.000000</td>\n",
       "      <td>1370.333333</td>\n",
       "      <td>1336.000000</td>\n",
       "      <td>1394.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>453.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>76.792443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>436.333333</td>\n",
       "      <td>403.500000</td>\n",
       "      <td>369.666667</td>\n",
       "      <td>321.5</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>387.666667</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>340.500000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>426.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>225.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>76.674654</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>751.666667</td>\n",
       "      <td>751.000000</td>\n",
       "      <td>805.0</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>715.333333</td>\n",
       "      <td>754.500000</td>\n",
       "      <td>690.500000</td>\n",
       "      <td>770.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>84.875056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>707 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            2000         2001         2002    2003         2004         2005   \n",
       "0    1276.666667  1099.500000  1084.000000  1092.5  1184.000000  1229.000000  \\\n",
       "1     124.000000    63.666667    53.000000    33.0    34.333333    14.500000   \n",
       "2     752.000000   895.500000   848.333333   914.0   989.000000   990.000000   \n",
       "3       0.000000   508.000000   597.333333   468.0   479.666667   568.500000   \n",
       "4    1737.500000  1456.500000  1010.000000  1042.0   987.666667  1021.500000   \n",
       "..           ...          ...          ...     ...          ...          ...   \n",
       "702   376.000000   397.000000   337.500000   434.0   487.000000   363.000000   \n",
       "703   609.500000   538.000000   522.500000   576.5   592.000000   611.000000   \n",
       "704  1310.000000  1389.000000  1533.666667  1601.0  1466.333333  1516.500000   \n",
       "705   436.333333   403.500000   369.666667   321.5   335.000000   387.666667   \n",
       "706   731.000000   751.666667   751.000000   805.0   793.000000   806.000000   \n",
       "\n",
       "            2006         2007         2008         2009  ...    9993    9994   \n",
       "0     986.000000  1100.500000  1146.000000  1073.500000  ...  1077.0  1173.0  \\\n",
       "1      76.666667    70.500000   101.500000   102.333333  ...   145.0   190.0   \n",
       "2    1047.000000   871.333333   855.000000   872.500000  ...   377.0   390.0   \n",
       "3     459.000000   451.666667   575.000000   550.333333  ...    22.0    30.0   \n",
       "4     979.000000   943.000000   921.000000   886.333333  ...   128.0   148.0   \n",
       "..           ...          ...          ...          ...  ...     ...     ...   \n",
       "702   424.000000   320.000000   340.333333   278.000000  ...   221.0   206.0   \n",
       "703   590.000000   457.500000   514.500000   454.000000  ...   205.5   251.0   \n",
       "704  1439.000000  1370.333333  1336.000000  1394.666667  ...   453.0   421.0   \n",
       "705   333.000000   340.500000   406.000000   426.500000  ...   225.0   259.0   \n",
       "706   715.333333   754.500000   690.500000   770.000000  ...   229.0   275.0   \n",
       "\n",
       "       9995    9996    9997    9998        9999  Oxacillin  Clindamycin   \n",
       "0    1163.0  1081.0  1032.0  1125.0  406.687788        0.0          0.0  \\\n",
       "1     276.0   235.0   226.0   215.0   64.410202        0.0          0.0   \n",
       "2     391.0   331.0   316.0   292.0  124.750563        0.0          1.0   \n",
       "3      19.0    31.0    36.0    20.0    4.279255        0.0          0.0   \n",
       "4     163.0   190.0   159.0    94.0   35.059556        0.0          0.0   \n",
       "..      ...     ...     ...     ...         ...        ...          ...   \n",
       "702   178.0   252.0   286.0   258.0   98.740466        0.0          0.0   \n",
       "703   241.0   227.0   186.0   169.0   61.124341        1.0          0.0   \n",
       "704   374.0   395.0   394.0   423.0   76.792443        0.0          0.0   \n",
       "705   310.0   279.0   210.0   297.0   76.674654        1.0          0.0   \n",
       "706   201.0   248.0   185.0   176.0   84.875056        0.0          0.0   \n",
       "\n",
       "     Fusidic acid  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             1.0  \n",
       "4             0.0  \n",
       "..            ...  \n",
       "702           0.0  \n",
       "703           0.0  \n",
       "704           1.0  \n",
       "705           0.0  \n",
       "706           0.0  \n",
       "\n",
       "[707 rows x 8003 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = input_folder+normalization+\"test_\"+bac+\".csv\"\n",
    "test_bac = pd.read_csv(test_file)\n",
    "test_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rXEshQwKQuzR",
    "outputId": "32756288-dd35-49f1-be9b-34bfe433baf7",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_bac[train_bac.columns.drop(list(train_bac.filter(regex='[^0-9]')))]\n",
    "test_x = test_bac[test_bac.columns.drop(list(test_bac.filter(regex='[^0-9]')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GiCCaGOEQuzS",
    "outputId": "bdc15429-4a19-4332-d59f-dd1c0ce37d88",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "antibiotics = train_bac.columns.drop(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_bac[antibiotics]\n",
    "test_y = test_bac[antibiotics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>432.500000</td>\n",
       "      <td>368.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>395.500000</td>\n",
       "      <td>336.333333</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>355.666667</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>411.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>260.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>88.186458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>1615.000000</td>\n",
       "      <td>1848.500000</td>\n",
       "      <td>1851.000000</td>\n",
       "      <td>2007.333333</td>\n",
       "      <td>1957.000000</td>\n",
       "      <td>2170.000000</td>\n",
       "      <td>1830.666667</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1598.000000</td>\n",
       "      <td>1646.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>409.306256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>364.000000</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>357.333333</td>\n",
       "      <td>372.500000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>335.333333</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>468.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>177.745583</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>517.000000</td>\n",
       "      <td>403.500000</td>\n",
       "      <td>503.333333</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>596.666667</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>455.500000</td>\n",
       "      <td>377.666667</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>698.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>509.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>142.596152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>908.000000</td>\n",
       "      <td>897.500000</td>\n",
       "      <td>836.333333</td>\n",
       "      <td>979.000000</td>\n",
       "      <td>1001.500000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>820.333333</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>802.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>349.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>116.520296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>1386.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1337.500000</td>\n",
       "      <td>1357.333333</td>\n",
       "      <td>1472.000000</td>\n",
       "      <td>1585.666667</td>\n",
       "      <td>1577.500000</td>\n",
       "      <td>1474.500000</td>\n",
       "      <td>1442.666667</td>\n",
       "      <td>1356.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>345.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>371.5</td>\n",
       "      <td>393.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>96.021886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>400.500000</td>\n",
       "      <td>366.333333</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>387.500000</td>\n",
       "      <td>368.500000</td>\n",
       "      <td>397.333333</td>\n",
       "      <td>356.500000</td>\n",
       "      <td>413.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.085550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>302.500000</td>\n",
       "      <td>268.333333</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>219.666667</td>\n",
       "      <td>191.500000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>231.500000</td>\n",
       "      <td>210.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>186.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>47.041503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>240.333333</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>348.333333</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>382.500000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>191.666667</td>\n",
       "      <td>281.500000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>46.533363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>883.000000</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>880.333333</td>\n",
       "      <td>907.500000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>925.000000</td>\n",
       "      <td>873.500000</td>\n",
       "      <td>829.666667</td>\n",
       "      <td>829.500000</td>\n",
       "      <td>779.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>257.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>94.004484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             2000         2001         2002         2003         2004   \n",
       "2428   432.500000   368.000000   388.000000   395.500000   336.333333  \\\n",
       "2277  1615.000000  1848.500000  1851.000000  2007.333333  1957.000000   \n",
       "1909   364.000000   434.000000   404.000000   321.000000   406.000000   \n",
       "826    517.000000   403.500000   503.333333   581.000000   596.666667   \n",
       "1779   908.000000   897.500000   836.333333   979.000000  1001.500000   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "861   1386.000000  1332.000000  1337.500000  1357.333333  1472.000000   \n",
       "2041     0.000000   400.500000   366.333333   349.000000   335.000000   \n",
       "118    302.500000   268.333333   260.000000   224.000000   219.666667   \n",
       "1626   240.333333   287.000000   348.333333   319.000000   382.500000   \n",
       "896    883.000000   893.000000   880.333333   907.500000   958.000000   \n",
       "\n",
       "             2005         2006         2007         2008         2009  ...   \n",
       "2428   358.000000   355.666667   340.000000   369.000000   411.666667  ...  \\\n",
       "2277  2170.000000  1830.666667  1800.000000  1598.000000  1646.000000  ...   \n",
       "1909   357.333333   372.500000   365.000000   335.333333   377.000000  ...   \n",
       "826    376.000000   455.500000   377.666667   523.000000   698.666667  ...   \n",
       "1779   904.000000   832.000000   820.333333   800.000000   802.000000  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "861   1585.666667  1577.500000  1474.500000  1442.666667  1356.500000  ...   \n",
       "2041   387.500000   368.500000   397.333333   356.500000   413.666667  ...   \n",
       "118    191.500000   230.000000   217.000000   231.500000   210.666667  ...   \n",
       "1626   412.000000   364.000000   191.666667   281.500000   272.000000  ...   \n",
       "896    925.000000   873.500000   829.666667   829.500000   779.000000  ...   \n",
       "\n",
       "        9993    9994    9995    9996    9997    9998        9999  Oxacillin   \n",
       "2428   260.0   249.0   304.0   263.0   231.0   248.0   88.186458        0.0  \\\n",
       "2277  1055.0  1129.0  1223.0  1238.0  1153.0  1060.0  409.306256        1.0   \n",
       "1909   468.0   499.0   544.0   570.0   507.0   439.0  177.745583        1.0   \n",
       "826    509.0   482.0   400.0   481.0   441.0   457.0  142.596152        0.0   \n",
       "1779   349.0   362.0   412.0   440.0   371.0   314.0  116.520296        0.0   \n",
       "...      ...     ...     ...     ...     ...     ...         ...        ...   \n",
       "861    345.0   350.0   300.0   371.5   393.0   316.0   96.021886        0.0   \n",
       "2041     2.0     9.0    17.0     4.0     6.0     9.0    1.085550        0.0   \n",
       "118    186.0   237.0   231.0   181.0   188.0   181.0   47.041503        0.0   \n",
       "1626    82.0    95.0   111.0    70.0   122.0   112.0   46.533363        0.0   \n",
       "896    257.0   218.0   251.0   254.0   284.0   264.0   94.004484        0.0   \n",
       "\n",
       "      Clindamycin  Fusidic acid  \n",
       "2428          0.0           1.0  \n",
       "2277          1.0           0.0  \n",
       "1909          0.0           0.0  \n",
       "826           0.0           0.0  \n",
       "1779          1.0           0.0  \n",
       "...           ...           ...  \n",
       "861           0.0           0.0  \n",
       "2041          0.0           0.0  \n",
       "118           1.0           0.0  \n",
       "1626          0.0           0.0  \n",
       "896           0.0           1.0  \n",
       "\n",
       "[1000 rows x 8003 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bac, _ = train_test_split(train_bac, train_size=1000, random_state=0, stratify=train_bac[antibiotics])\n",
    "train_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_bac[train_bac.columns.drop(list(train_bac.filter(regex='[^0-9]')))]\n",
    "train_y = train_bac[antibiotics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/features/\"+bac+\"_selected_features.txt\") as file:\n",
    "    selected_features = file.read().split(\",\")\n",
    "selected_features.pop()\n",
    "\n",
    "train_x = train_x[selected_features]\n",
    "test_x = test_x[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multilabel_f1_wrapper(true, pred, average=\"weighted\"):\n",
    "    if isinstance(true, list):\n",
    "        true = np.array(true)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        true = true.to_numpy()\n",
    "    if isinstance(pred, list):\n",
    "        pred = np.array(pred)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        pred = pred.to_numpy()\n",
    "    column = 0\n",
    "    total = 0\n",
    "    while column < true[0].size:\n",
    "        total+=f1_score(true[:, column], pred[:, column], average=average)\n",
    "        column+=1\n",
    "    return total/(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def report(true, pred):\n",
    "        \n",
    "    hl = hamming_loss(true, pred)\n",
    "    f1w = multilabel_f1_wrapper(true, pred, \"weighted\")\n",
    "    acc = accuracy_score(true, pred)\n",
    "    \n",
    "    f1u = multilabel_f1_wrapper(true, pred, \"macro\")\n",
    "    f1su = f1_score(true, pred, average=\"macro\")\n",
    "    f1sw = f1_score(true, pred, average=\"weighted\")\n",
    "\n",
    "    \n",
    "    print(\"Main metrics:\")\n",
    "    print(\" Hamming Loss:\", hl)\n",
    "    print(\" Accuracy:\", acc)\n",
    "    print(\" F1 Score (Weighted):\", f1w)\n",
    "    print(\"================================================\")\n",
    "    print(\"Other metrics:\")\n",
    "    print(\" F1 Score (Unweighted):\", f1u)\n",
    "    print(\" F1 Score (sklearn Unweighted):\", f1su)\n",
    "    print(\" F1 Score (sklearn Weighted):\", f1sw)\n",
    "    return hl, acc, f1w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-gP_gv8QuzZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = ClassifierChain(TabPFNClassifier(device='cpu', N_ensemble_configurations=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cv = cross_validate(model, train_x, train_y, scoring=make_scorer(multilabel_f1_wrapper), cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split scores:\n",
      " 0 0.8204744721042697\n",
      " 1 0.8635905190745795\n",
      " 2 0.8568072484613118\n",
      " 3 0.8805653254120221\n",
      " 4 0.8517859619269371\n",
      "Mean score: 0.8546447053958242\n"
     ]
    }
   ],
   "source": [
    "print(\"Split scores:\")\n",
    "for i in range(0, 5):\n",
    "    print(\"\", i, cv[\"test_score\"][i])\n",
    "    \n",
    "print(\"Mean score:\", cv[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main metrics:\n",
      " Hamming Loss: 0.10325318246110325\n",
      " Accuracy: 0.7298444130127298\n",
      " F1 Score (Weighted): 0.861973926672072\n",
      "================================================\n",
      "Other metrics:\n",
      " F1 Score (Unweighted): 0.588405213459765\n",
      " F1 Score (sklearn Unweighted): 0.23415977961432502\n",
      " F1 Score (sklearn Weighted): 0.3451628585318424\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y)\n",
    "pred = model.predict(test_x)\n",
    "model_hl, model_acc, model_f1 = report(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tabpfn_s_aureus_raw.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, 'tabpfn_s_aureus_raw.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, len(antibiotics), figsize=(len(antibiotics)*5, 5))\n",
    "fig.supxlabel(\"Predicted Label\")\n",
    "fig.supylabel(\"True Label\")\n",
    "\n",
    "cm_svm_c = multilabel_confusion_matrix(test_y, pred)\n",
    "\n",
    "for i in range(len(antibiotics)):\n",
    "  sns.heatmap(ax=axes[i], data=cm_svm_c[i], annot=True, fmt='d', cbar=None, cmap=\"Blues\", xticklabels=[\"S\", \"R\"], yticklabels=[\"S\", \"R\"]).set(title=antibiotics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/eocque/bac/venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22626767, 0.07229915, 0.03211896],\n",
       "       [0.05015613, 0.058298  , 0.03032763],\n",
       "       [0.01000705, 0.09439566, 0.03167585],\n",
       "       ...,\n",
       "       [0.42799827, 0.07222836, 0.03858757],\n",
       "       [0.15163654, 0.04695556, 0.03137895],\n",
       "       [0.04837862, 0.08124733, 0.02317812]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = model.predict_proba(test_x)\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for antibiotic Oxacillin\n",
      " Mean TP: 0.817624319300932\n",
      " Mean TN: 0.0806918331798674\n",
      " Mean FP: 0.7172300656636555\n",
      " Mean FN: 0.18302350824600772\n",
      "Results for antibiotic Clindamycin\n",
      " Mean TP: None\n",
      " Mean TN: 0.08687989304134669\n",
      " Mean FP: None\n",
      " Mean FN: 0.12787390346912778\n",
      "Results for antibiotic Fusidic acid\n",
      " Mean TP: None\n",
      " Mean TN: 0.03349181066838366\n",
      " Mean FP: None\n",
      " Mean FN: 0.03762463625106546\n"
     ]
    }
   ],
   "source": [
    "for antibiotic in range(len(antibiotics)):\n",
    "    count_tp = 0\n",
    "    count_tn = 0\n",
    "    count_fp = 0\n",
    "    count_fn = 0\n",
    "    sum_tp = 0\n",
    "    sum_tn = 0\n",
    "    sum_fp = 0\n",
    "    sum_fn = 0\n",
    "    for i in range(len(proba[:, antibiotic])):\n",
    "        disc_pred = int(proba[i, antibiotic] > 0.5)\n",
    "        if disc_pred == 1:\n",
    "            if disc_pred == test_y.iloc[i, antibiotic]:\n",
    "                count_tp += 1\n",
    "                sum_tp += proba[i, antibiotic]\n",
    "            else:\n",
    "                count_fp += 1\n",
    "                sum_fp += proba[i, antibiotic]\n",
    "        else:\n",
    "            if disc_pred == test_y.iloc[i, antibiotic]:\n",
    "                count_tn += 1\n",
    "                sum_tn += proba[i, antibiotic]\n",
    "            else:\n",
    "                count_fn += 1\n",
    "                sum_fn += proba[i, antibiotic]\n",
    "    print(\"Results for antibiotic\", antibiotics[antibiotic])\n",
    "    if count_tp == 0:\n",
    "        print(\" Mean TP: None\")\n",
    "    else: \n",
    "        print(\" Mean TP:\", sum_tp/count_tp)\n",
    "    if count_tn == 0:\n",
    "        print(\" Mean TN: None\")\n",
    "    else: \n",
    "        print(\" Mean TN:\", sum_tn/count_tn)\n",
    "    if count_fp == 0:\n",
    "        print(\" Mean FP: None\")\n",
    "    else: \n",
    "        print(\" Mean FP:\", sum_fp/count_fp)\n",
    "    if count_fn == 0:\n",
    "        print(\" Mean FN: None\")\n",
    "    else: \n",
    "        print(\" Mean FN:\", sum_fn/count_fn)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
