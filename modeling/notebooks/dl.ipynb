{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/castudil/bacteria-multi-label/blob/main/multilabel_bac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKUeIuZcpHUl"
   },
   "source": [
    "Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bzVprbfpWSLa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import (f1_score, multilabel_confusion_matrix,\n",
    "                             accuracy_score, hamming_loss, jaccard_score, make_scorer)\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from joblib import dump, load\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "nQsWnulDWdDD",
    "outputId": "6f97687c-b560-4e34-fd8b-9c3ef7aeb0c7",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.016147</td>\n",
       "      <td>0.016983</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.019405</td>\n",
       "      <td>0.023356</td>\n",
       "      <td>0.026224</td>\n",
       "      <td>0.026569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037966</td>\n",
       "      <td>0.030364</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>0.040851</td>\n",
       "      <td>0.034176</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.025638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009001</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014496</td>\n",
       "      <td>0.024966</td>\n",
       "      <td>0.027437</td>\n",
       "      <td>0.026541</td>\n",
       "      <td>0.022940</td>\n",
       "      <td>0.020572</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>0.020910</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.021436</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.020229</td>\n",
       "      <td>0.018818</td>\n",
       "      <td>0.018637</td>\n",
       "      <td>0.018815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024620</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.026715</td>\n",
       "      <td>0.032045</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>0.029085</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.016407</td>\n",
       "      <td>0.018011</td>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.019116</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051312</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.049338</td>\n",
       "      <td>0.055039</td>\n",
       "      <td>0.054541</td>\n",
       "      <td>0.058643</td>\n",
       "      <td>0.058919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236769</td>\n",
       "      <td>0.217499</td>\n",
       "      <td>0.187244</td>\n",
       "      <td>0.216243</td>\n",
       "      <td>0.221910</td>\n",
       "      <td>0.226531</td>\n",
       "      <td>0.221965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>0.056616</td>\n",
       "      <td>0.039011</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>0.048517</td>\n",
       "      <td>0.050865</td>\n",
       "      <td>0.047771</td>\n",
       "      <td>0.049312</td>\n",
       "      <td>0.048257</td>\n",
       "      <td>0.049417</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021169</td>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.021037</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>0.125837</td>\n",
       "      <td>0.107712</td>\n",
       "      <td>0.109186</td>\n",
       "      <td>0.107613</td>\n",
       "      <td>0.109855</td>\n",
       "      <td>0.105060</td>\n",
       "      <td>0.099640</td>\n",
       "      <td>0.104169</td>\n",
       "      <td>0.120303</td>\n",
       "      <td>0.125067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082375</td>\n",
       "      <td>0.083446</td>\n",
       "      <td>0.096510</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.092228</td>\n",
       "      <td>0.085599</td>\n",
       "      <td>0.042142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035603</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>0.042372</td>\n",
       "      <td>0.046666</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.043914</td>\n",
       "      <td>0.039875</td>\n",
       "      <td>0.037170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.006124</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049241</td>\n",
       "      <td>0.039586</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.039139</td>\n",
       "      <td>0.046816</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.022810</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>0.026694</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.025140</td>\n",
       "      <td>0.026609</td>\n",
       "      <td>0.024203</td>\n",
       "      <td>0.020953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090198</td>\n",
       "      <td>0.101889</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>0.122065</td>\n",
       "      <td>0.095740</td>\n",
       "      <td>0.082289</td>\n",
       "      <td>0.081714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2824 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2000      2001      2002      2003      2004      2005      2006   \n",
       "0     0.018721  0.016147  0.016983  0.021218  0.020846  0.019784  0.019405  \\\n",
       "1     0.009001  0.007475  0.006874  0.008575  0.009539  0.007894  0.008314   \n",
       "2     0.022354  0.020220  0.020910  0.024631  0.021436  0.021197  0.020229   \n",
       "3     0.017619  0.016073  0.016407  0.018011  0.019364  0.018950  0.017607   \n",
       "4     0.008264  0.008229  0.006753  0.006657  0.010107  0.007039  0.008250   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2819  0.056616  0.039011  0.040380  0.048517  0.050865  0.047771  0.049312   \n",
       "2820  0.125837  0.107712  0.109186  0.107613  0.109855  0.105060  0.099640   \n",
       "2821  0.000000  0.000000  0.035603  0.039994  0.042372  0.046666  0.045781   \n",
       "2822  0.005443  0.005998  0.003670  0.005588  0.006124  0.005019  0.004853   \n",
       "2823  0.026184  0.022810  0.023200  0.020464  0.026694  0.024624  0.025140   \n",
       "\n",
       "          2007      2008      2009  ...      9993      9994      9995   \n",
       "0     0.023356  0.026224  0.026569  ...  0.037966  0.030364  0.037545  \\\n",
       "1     0.008013  0.008664  0.008923  ...  0.014496  0.024966  0.027437   \n",
       "2     0.018818  0.018637  0.018815  ...  0.024620  0.022942  0.026715   \n",
       "3     0.019116  0.023623  0.024492  ...  0.051312  0.047458  0.049338   \n",
       "4     0.010670  0.008134  0.006513  ...  0.236769  0.217499  0.187244   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2819  0.048257  0.049417  0.049000  ...  0.021169  0.023617  0.033694   \n",
       "2820  0.104169  0.120303  0.125067  ...  0.082375  0.083446  0.096510   \n",
       "2821  0.043914  0.039875  0.037170  ...  0.006903  0.008322  0.011071   \n",
       "2822  0.005400  0.004169  0.005151  ...  0.049241  0.039586  0.050542   \n",
       "2823  0.026609  0.024203  0.020953  ...  0.090198  0.101889  0.105174   \n",
       "\n",
       "          9996      9997      9998      9999  Oxacillin  Clindamycin   \n",
       "0     0.040851  0.034176  0.046110  0.025638        0.0          0.0  \\\n",
       "1     0.026541  0.022940  0.020572  0.032504        0.0          0.0   \n",
       "2     0.032045  0.030431  0.029085  0.013117        0.0          0.0   \n",
       "3     0.055039  0.054541  0.058643  0.058919        0.0          0.0   \n",
       "4     0.216243  0.221910  0.226531  0.221965        0.0          0.0   \n",
       "...        ...       ...       ...       ...        ...          ...   \n",
       "2819  0.021037  0.018727  0.010641  0.009238        0.0          1.0   \n",
       "2820  0.084883  0.092228  0.085599  0.042142        0.0          1.0   \n",
       "2821  0.010274  0.004682  0.003547  0.001744        0.0          0.0   \n",
       "2822  0.039139  0.046816  0.043036  0.037402        1.0          1.0   \n",
       "2823  0.122065  0.095740  0.082289  0.081714        0.0          0.0   \n",
       "\n",
       "      Fusidic acid  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  \n",
       "...            ...  \n",
       "2819           0.0  \n",
       "2820           0.0  \n",
       "2821           0.0  \n",
       "2822           0.0  \n",
       "2823           0.0  \n",
       "\n",
       "[2824 rows x 8003 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = \"data/processed/raw/train_s_aureus_driams.csv\"\n",
    "train_bac = pd.read_csv(train_file)\n",
    "train_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044453</td>\n",
       "      <td>0.032486</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>0.034223</td>\n",
       "      <td>0.037528</td>\n",
       "      <td>0.039503</td>\n",
       "      <td>0.031378</td>\n",
       "      <td>0.035506</td>\n",
       "      <td>0.037688</td>\n",
       "      <td>0.035658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247814</td>\n",
       "      <td>0.263833</td>\n",
       "      <td>0.279904</td>\n",
       "      <td>0.264432</td>\n",
       "      <td>0.241573</td>\n",
       "      <td>0.266020</td>\n",
       "      <td>0.231517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004318</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033364</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>0.066426</td>\n",
       "      <td>0.057485</td>\n",
       "      <td>0.052903</td>\n",
       "      <td>0.050839</td>\n",
       "      <td>0.036631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026459</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.028609</td>\n",
       "      <td>0.031314</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.028051</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>0.028978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.094103</td>\n",
       "      <td>0.080969</td>\n",
       "      <td>0.073970</td>\n",
       "      <td>0.069047</td>\n",
       "      <td>0.070988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>0.014582</td>\n",
       "      <td>0.015084</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.018272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.007583</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060499</td>\n",
       "      <td>0.043034</td>\n",
       "      <td>0.030296</td>\n",
       "      <td>0.032635</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>0.032762</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>0.030382</td>\n",
       "      <td>0.030233</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029452</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>0.039230</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>0.037219</td>\n",
       "      <td>0.022227</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.013092</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.009902</td>\n",
       "      <td>0.013513</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.010994</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050851</td>\n",
       "      <td>0.046334</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.066948</td>\n",
       "      <td>0.061007</td>\n",
       "      <td>0.056178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.021222</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>0.015512</td>\n",
       "      <td>0.017995</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.019427</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047285</td>\n",
       "      <td>0.056455</td>\n",
       "      <td>0.058002</td>\n",
       "      <td>0.055528</td>\n",
       "      <td>0.043539</td>\n",
       "      <td>0.039962</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>0.045613</td>\n",
       "      <td>0.041040</td>\n",
       "      <td>0.046177</td>\n",
       "      <td>0.050216</td>\n",
       "      <td>0.046525</td>\n",
       "      <td>0.048843</td>\n",
       "      <td>0.045920</td>\n",
       "      <td>0.044283</td>\n",
       "      <td>0.043983</td>\n",
       "      <td>0.046330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104234</td>\n",
       "      <td>0.094692</td>\n",
       "      <td>0.090012</td>\n",
       "      <td>0.096624</td>\n",
       "      <td>0.092228</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.043682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.015193</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>0.010474</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.014157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051772</td>\n",
       "      <td>0.058255</td>\n",
       "      <td>0.074609</td>\n",
       "      <td>0.068249</td>\n",
       "      <td>0.049157</td>\n",
       "      <td>0.070229</td>\n",
       "      <td>0.043615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.022209</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.025181</td>\n",
       "      <td>0.025069</td>\n",
       "      <td>0.025761</td>\n",
       "      <td>0.022690</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052692</td>\n",
       "      <td>0.061853</td>\n",
       "      <td>0.048375</td>\n",
       "      <td>0.060665</td>\n",
       "      <td>0.043305</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>0.048284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>707 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2000      2001      2002      2003      2004      2005      2006   \n",
       "0    0.044453  0.032486  0.032540  0.034223  0.037528  0.039503  0.031378  \\\n",
       "1    0.004318  0.001881  0.001274  0.000902  0.000892  0.000049  0.002188   \n",
       "2    0.026184  0.026459  0.025393  0.028609  0.031314  0.031739  0.033337   \n",
       "3    0.000000  0.015010  0.017782  0.014582  0.015084  0.018046  0.014461   \n",
       "4    0.060499  0.043034  0.030296  0.032635  0.031272  0.032762  0.031154   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "702  0.013092  0.011730  0.009902  0.013513  0.015317  0.011370  0.013338   \n",
       "703  0.021222  0.015896  0.015512  0.017995  0.018663  0.019427  0.018667   \n",
       "704  0.045613  0.041040  0.046177  0.050216  0.046525  0.048843  0.045920   \n",
       "705  0.015193  0.011922  0.010877  0.009975  0.010474  0.012171  0.010417   \n",
       "706  0.025453  0.022209  0.022442  0.025181  0.025069  0.025761  0.022690   \n",
       "\n",
       "         2007      2008      2009  ...      9993      9994      9995   \n",
       "0    0.035506  0.037688  0.035658  ...  0.247814  0.263833  0.279904  \\\n",
       "1    0.002001  0.003081  0.003384  ...  0.033364  0.042735  0.066426   \n",
       "2    0.028051  0.028047  0.028978  ...  0.086746  0.087719  0.094103   \n",
       "3    0.014400  0.018769  0.018272  ...  0.005062  0.006748  0.004573   \n",
       "4    0.030382  0.030233  0.029438  ...  0.029452  0.033288  0.039230   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "702  0.010117  0.010994  0.009222  ...  0.050851  0.046334  0.042840   \n",
       "703  0.014589  0.016765  0.015071  ...  0.047285  0.056455  0.058002   \n",
       "704  0.044283  0.043983  0.046330  ...  0.104234  0.094692  0.090012   \n",
       "705  0.010783  0.013170  0.014157  ...  0.051772  0.058255  0.074609   \n",
       "706  0.024250  0.022596  0.025572  ...  0.052692  0.061853  0.048375   \n",
       "\n",
       "         9996      9997      9998      9999  Oxacillin  Clindamycin   \n",
       "0    0.264432  0.241573  0.266020  0.231517        0.0          0.0  \\\n",
       "1    0.057485  0.052903  0.050839  0.036631        0.0          0.0   \n",
       "2    0.080969  0.073970  0.069047  0.070988        0.0          1.0   \n",
       "3    0.007583  0.008427  0.004729  0.002394        0.0          0.0   \n",
       "4    0.046477  0.037219  0.022227  0.019920        0.0          0.0   \n",
       "..        ...       ...       ...       ...        ...          ...   \n",
       "702  0.061644  0.066948  0.061007  0.056178        0.0          0.0   \n",
       "703  0.055528  0.043539  0.039962  0.034761        1.0          0.0   \n",
       "704  0.096624  0.092228  0.100024  0.043682        0.0          0.0   \n",
       "705  0.068249  0.049157  0.070229  0.043615        1.0          0.0   \n",
       "706  0.060665  0.043305  0.041617  0.048284        0.0          0.0   \n",
       "\n",
       "     Fusidic acid  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             1.0  \n",
       "4             0.0  \n",
       "..            ...  \n",
       "702           0.0  \n",
       "703           0.0  \n",
       "704           1.0  \n",
       "705           0.0  \n",
       "706           0.0  \n",
       "\n",
       "[707 rows x 8003 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = \"data/processed/raw/test_s_aureus_driams.csv\"\n",
    "test_bac = pd.read_csv(test_file)\n",
    "test_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rXEshQwKQuzR",
    "outputId": "32756288-dd35-49f1-be9b-34bfe433baf7",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_bac[train_bac.columns.drop(list(train_bac.filter(regex='[^0-9]')))]\n",
    "test_x = test_bac[test_bac.columns.drop(list(test_bac.filter(regex='[^0-9]')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GiCCaGOEQuzS",
    "outputId": "bdc15429-4a19-4332-d59f-dd1c0ce37d88",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "antibiotics = train_bac.columns.drop(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_bac[antibiotics]\n",
    "test_y = test_bac[antibiotics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tensor = torch.FloatTensor(train_x.to_numpy())\n",
    "train_y_tensor = torch.LongTensor(train_y[\"Oxacillin\"].to_numpy())\n",
    "test_x_tensor = torch.FloatTensor(test_x.to_numpy())\n",
    "test_y_tensor = torch.LongTensor(test_y[\"Oxacillin\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BacteriaDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BacteriaDataset(train_x_tensor, train_y_tensor)\n",
    "test_dataset = BacteriaDataset(test_x_tensor, test_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear_relu_stack(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = len(train_x.columns)\n",
    "output_dim = len(np.unique(train_y))\n",
    "model = NeuralNetworkClassificationModel(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print(batch)\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.036095  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.055754  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.104066  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.105920  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.082711  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.361223 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.053719  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.061618  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.068064  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.040853  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.072162  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.418862 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.025805  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.042629  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.088553  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.070769  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.044931  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337535 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.038643  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.059210  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.041099  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.036805  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.039695  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.334212 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.056800  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.114145  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.049679  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.043400  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.067878  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.402175 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.041095  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.118853  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.047977  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.087639  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.087507  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.335241 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.044950  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.069976  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.039494  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.167359  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.033309  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.363895 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.031061  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.035221  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.053119  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.032396  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.068355  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.401462 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.035491  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.084559  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.080471  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.082216  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.035730  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337085 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.023774  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.018151  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.030454  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.068216  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.077181  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.335988 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.060344  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.052688  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.050971  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.062120  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.065730  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.335132 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.031631  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.116310  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.039198  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.049285  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.080893  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.416520 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.094842  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.049403  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.034456  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.078180  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.092291  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.336199 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.067120  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.028240  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.048225  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.081861  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.049716  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.335982 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.057091  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.037061  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.014715  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.050247  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.055555  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.335330 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.092092  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.189046  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.048194  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.018397  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.047788  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.334840 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.069276  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.022016  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.028287  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.079257  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.062291  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.339034 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.074098  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.060219  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.061962  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.027681  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.017862  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.336062 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.090774  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.046494  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.111438  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.035316  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.073907  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337713 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.035146  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.045312  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.030366  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.071072  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.121740  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.386020 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.064618  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.149022  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.024288  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.052316  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.068749  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337805 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.089478  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.053717  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.060412  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.043400  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.039333  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.336872 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.059925  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.064616  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.062751  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.122276  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.131950  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337987 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.038397  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.146680  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.060952  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.033913  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.076796  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.338585 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.117250  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.081032  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.142878  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.044927  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.074166  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.335246 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.071722  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.072666  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.039207  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.181488  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.055129  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.335662 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.052227  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.044035  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.141115  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.036361  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.084235  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.335696 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.138162  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.056752  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.058429  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.059534  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.092688  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.336540 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.114346  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.035592  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.037610  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.080025  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.038810  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.341618 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.066726  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.019717  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.074329  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.095061  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.032344  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337435 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.022621  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.023808  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.042264  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.046666  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.049532  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.338727 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.046392  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.037936  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.079889  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.067071  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.070463  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.336492 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.019427  [   64/ 2824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Loss: 0.105259  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.125956  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.112142  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.061841  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.437109 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.073299  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.068720  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.040497  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.037913  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.040317  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.338874 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.054370  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.035698  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.037612  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.045491  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.067575  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.336391 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.079877  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.061422  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.146791  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.051612  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.055910  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.449353 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.036854  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.047261  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.042031  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.121303  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.031887  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.344714 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.104469  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.041794  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.092965  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.047526  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.057605  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.337404 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.040020  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.030477  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.070769  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.043755  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.051591  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.337202 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.070121  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.065826  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.088506  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.048599  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.079499  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.454974 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.074975  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.068483  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.060417  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.065357  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.065967  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.340849 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.057790  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.033554  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.056891  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.046463  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.033713  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.336663 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.017656  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.037536  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.077181  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.090273  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.093377  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337196 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.028277  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.049444  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.032140  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.070984  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.061511  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337092 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.036214  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.028542  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.045104  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.051283  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.053751  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337781 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.107994  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.074420  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.029496  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.032794  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.044045  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337770 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.052569  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.060650  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.033167  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.040671  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.093819  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.560044 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.025531  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.038385  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.051391  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.055643  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.031378  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.338945 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.112888  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.021826  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.063572  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.042714  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.118213  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.336890 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.053405  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.019376  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.017742  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.058210  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.115243  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.336819 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.101694  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.086691  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.054780  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.104787  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.051050  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.342202 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.068034  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.046180  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.051375  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.087624  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.038374  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337099 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.049314  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.036396  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.063489  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.064385  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.078334  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.423163 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.039614  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.101684  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.054545  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.075294  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.027065  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.337813 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.130812  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.030192  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.040278  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.042056  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.062654  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.338293 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.026528  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.049096  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.078023  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.074152  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.051717  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.338706 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.089542  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.083485  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.081914  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.024519  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.062382  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.342594 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.056381  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.046216  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.120095  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.062720  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.054532  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.338372 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.040693  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.102119  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.112700  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.059905  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.087904  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.340657 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.078419  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.090083  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.070868  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.071437  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.052878  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.341147 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.148766  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.057866  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.115768  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.042760  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.018816  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.337583 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.058505  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.034366  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.061197  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.099606  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.055476  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.418613 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.064372  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.093124  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.056178  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.020150  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.045092  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.337837 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.036982  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.099944  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.027373  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.096793  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.045868  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.338452 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.102966  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.025113  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.039812  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.046713  [ 1984/ 2824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Loss: 0.059665  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.338148 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.111756  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.096104  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.038139  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.072330  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.015943  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.339235 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.064416  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.045532  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.172003  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.129772  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.047040  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.401866 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.114997  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.027294  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.048927  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.041594  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.066924  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.422296 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.043304  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.020999  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.078229  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.020890  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.109651  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.448320 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.055511  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.101415  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.062385  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.132765  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.030930  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.345660 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.028928  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.022774  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.040803  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.091402  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.012514  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.350035 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.055375  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.044597  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.056884  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.050577  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.050061  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.337898 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.053062  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.056023  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.062378  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.022881  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.039769  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.339840 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.049561  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.064464  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.064448  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.037865  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.092094  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.338690 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.090822  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.053866  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.020492  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.028000  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.079880  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.387905 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.166999  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.035030  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.121049  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.042417  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.025696  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.339538 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.081297  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.023671  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.027552  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.072298  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.128853  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.486278 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.064998  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.030568  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.091513  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.153223  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.072058  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.342245 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.096212  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.026760  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.063231  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.061382  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.080069  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.338652 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.042866  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.038433  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.060958  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.044100  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.036149  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.394399 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.053361  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.025313  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.083344  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.069780  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.058833  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.340740 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.035368  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.081980  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.114098  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.098794  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.059041  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.423796 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.033776  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.072245  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.136603  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.064930  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.039373  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.341571 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.080176  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.058176  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.040884  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.147482  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.164919  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.340262 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.056392  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.039847  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.096221  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.089381  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.030084  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.345899 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.040994  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.059541  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.098817  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.030039  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.113530  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.443894 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.050073  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.049149  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.074682  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.040655  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.067141  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.338809 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.034716  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.062487  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.052777  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.143739  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.046520  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.390849 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.030343  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.034389  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.057851  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.021859  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.087079  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.339384 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.061897  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.063582  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.150479  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.059470  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.077674  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.340411 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.041329  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.089842  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.081569  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.030914  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.041292  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.346411 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.079412  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.152813  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.054931  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.054735  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.060812  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.342048 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.057094  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.055723  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.095430  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.018496  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.090952  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.486346 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.049724  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.039123  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.045321  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.048149  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.040662  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.366627 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.066962  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.089740  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.062079  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.038816  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.087878  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.339664 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.075455  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.071031  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.050385  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.075716  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.035763  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.359257 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.053979  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.048436  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.101822  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.024619  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.013984  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.381171 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.039784  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.039365  [  704/ 2824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Loss: 0.036672  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.033114  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.066379  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.341119 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.021369  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.038266  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.055385  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.039993  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.037112  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.421687 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "0\n",
      "Loss: 0.090615  [   64/ 2824]\n",
      "10\n",
      "Loss: 0.068235  [  704/ 2824]\n",
      "20\n",
      "Loss: 0.144229  [ 1344/ 2824]\n",
      "30\n",
      "Loss: 0.061213  [ 1984/ 2824]\n",
      "40\n",
      "Loss: 0.055019  [ 2624/ 2824]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.522031 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = []\n",
    "predictions_test =  []\n",
    "with torch.no_grad():\n",
    "    predictions_train = model(train_x_tensor)\n",
    "    predictions_test = model(test_x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_multiclass(pred_arr,original_arr):\n",
    "    if len(pred_arr)!=len(original_arr):\n",
    "        return False\n",
    "    pred_arr = pred_arr.numpy()\n",
    "    original_arr = original_arr.numpy()\n",
    "    final_pred= []\n",
    "   \n",
    "    for i in range(len(pred_arr)):\n",
    "        final_pred.append(np.argmax(pred_arr[i]))\n",
    "    final_pred = np.array(final_pred)\n",
    "    count = 0\n",
    "    #here we are doing a simple comparison between the predicted_arr and the original_arr to get the final accuracy\n",
    "    for i in range(len(original_arr)):\n",
    "        if final_pred[i] == original_arr[i]:\n",
    "            count+=1\n",
    "    return count/len(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = get_accuracy_multiclass(predictions_train,train_y_tensor)\n",
    "test_acc  = get_accuracy_multiclass(predictions_test,test_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.123\n",
      "Test Accuracy: 89.958\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Accuracy: {round(train_acc*100,3)}\")\n",
    "print(f\"Test Accuracy: {round(test_acc*100,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bayesopt = BayesSearchCV(\n",
    "#     ClassifierChain(xgb.XGBClassifier(), random_state=0),\n",
    "#     {\n",
    "#         \"base_estimator__objective\": Categorical([\"binary:logistic\"]),\n",
    "#         \"base_estimator__max_depth\": Integer(1, 10),\n",
    "#         \"base_estimator__min_child_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "#         \"base_estimator__max_delta_step\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "#         \"base_estimator__subsample\": Real(1e-6, 1, prior=\"log-uniform\"),\n",
    "#         \"base_estimator__tree_method\": Categorical([\"exact\", \"approx\", \"hist\"]),\n",
    "#         \"base_estimator__scale_pos_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "#         \"base_estimator__gamma\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "#         \"base_estimator__eta\": Real(1e-6, 1, prior=\"log-uniform\")\n",
    "#     },\n",
    "#     n_iter=250,\n",
    "#     cv=5,\n",
    "#     random_state=0,\n",
    "#     n_jobs=1,\n",
    "#     n_points=1,\n",
    "#     scoring=make_scorer(multilabel_f1_wrapper),\n",
    "#     verbose=1,\n",
    "# )\n",
    "\n",
    "# bayesopt.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_iteration = 0\n",
    "# for i in range(0, 250):\n",
    "#     if bayesopt.cv_results_[\"mean_test_score\"][i] == bayesopt.best_score_:\n",
    "#         best_iteration = i\n",
    "# print(\"Best iteration:\", best_iteration)\n",
    "# print(\"Split scores:\")\n",
    "# for i in range(0, 5):\n",
    "#     print(\"\", i, bayesopt.cv_results_[\"split\"+str(i)+\"_test_score\"][best_iteration])\n",
    "    \n",
    "# print(\"Mean score:\", bayesopt.best_score_)\n",
    "# print(\"Best parameter combination found:\", bayesopt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = bayesopt.best_estimator_\n",
    "# model.fit(train_x, train_y) \n",
    "# pred = model.predict_proba(test_x)\n",
    "# model_hl, model_acc, model_f1 = report(test_y, (pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(model, 'nn_s_aureus_raw.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, len(antibiotics), figsize=(len(antibiotics)*5, 5))\n",
    "# fig.supxlabel(\"Predicted Label\")\n",
    "# fig.supylabel(\"True Label\")\n",
    "\n",
    "# cm_svm_c = multilabel_confusion_matrix(test_y, (pred > 0.5))\n",
    "\n",
    "# for i in range(len(antibiotics)):\n",
    "#   sns.heatmap(ax=axes[i], data=cm_svm_c[i], annot=True, fmt='d', cbar=None, cmap=\"Blues\", xticklabels=[\"S\", \"R\"], yticklabels=[\"S\", \"R\"]).set(title=antibiotics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
