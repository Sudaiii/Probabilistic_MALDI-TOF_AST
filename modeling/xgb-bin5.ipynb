{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/castudil/bacteria-multi-label/blob/main/multilabel_bac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKUeIuZcpHUl"
   },
   "source": [
    "Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bzVprbfpWSLa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import (f1_score, multilabel_confusion_matrix,\n",
    "                             accuracy_score, hamming_loss, jaccard_score, make_scorer)\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from joblib import dump, load\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "nQsWnulDWdDD",
    "outputId": "6f97687c-b560-4e34-fd8b-9c3ef7aeb0c7",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2005</th>\n",
       "      <th>2010</th>\n",
       "      <th>2015</th>\n",
       "      <th>2020</th>\n",
       "      <th>2025</th>\n",
       "      <th>2030</th>\n",
       "      <th>2035</th>\n",
       "      <th>2040</th>\n",
       "      <th>2045</th>\n",
       "      <th>...</th>\n",
       "      <th>9965</th>\n",
       "      <th>9970</th>\n",
       "      <th>9975</th>\n",
       "      <th>9980</th>\n",
       "      <th>9985</th>\n",
       "      <th>9990</th>\n",
       "      <th>9995</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021589</td>\n",
       "      <td>0.022946</td>\n",
       "      <td>0.020381</td>\n",
       "      <td>0.019762</td>\n",
       "      <td>0.022195</td>\n",
       "      <td>0.020017</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>0.023774</td>\n",
       "      <td>0.026180</td>\n",
       "      <td>0.022159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033513</td>\n",
       "      <td>0.041287</td>\n",
       "      <td>0.036239</td>\n",
       "      <td>0.030211</td>\n",
       "      <td>0.035161</td>\n",
       "      <td>0.033998</td>\n",
       "      <td>0.038221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009301</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.007711</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.007348</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>0.008332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023975</td>\n",
       "      <td>0.026137</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>0.024854</td>\n",
       "      <td>0.029517</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.025007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025274</td>\n",
       "      <td>0.019463</td>\n",
       "      <td>0.019516</td>\n",
       "      <td>0.020692</td>\n",
       "      <td>0.018069</td>\n",
       "      <td>0.018431</td>\n",
       "      <td>0.023269</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.020079</td>\n",
       "      <td>0.020810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025799</td>\n",
       "      <td>0.021951</td>\n",
       "      <td>0.029973</td>\n",
       "      <td>0.025818</td>\n",
       "      <td>0.029609</td>\n",
       "      <td>0.024297</td>\n",
       "      <td>0.027897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020099</td>\n",
       "      <td>0.020638</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>0.018159</td>\n",
       "      <td>0.018335</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0.065560</td>\n",
       "      <td>0.026733</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>0.027320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050939</td>\n",
       "      <td>0.053343</td>\n",
       "      <td>0.051328</td>\n",
       "      <td>0.053429</td>\n",
       "      <td>0.050428</td>\n",
       "      <td>0.047393</td>\n",
       "      <td>0.054734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.008043</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.013516</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.008767</td>\n",
       "      <td>0.007783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158928</td>\n",
       "      <td>0.170674</td>\n",
       "      <td>0.173528</td>\n",
       "      <td>0.182519</td>\n",
       "      <td>0.194263</td>\n",
       "      <td>0.213917</td>\n",
       "      <td>0.213849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>0.054429</td>\n",
       "      <td>0.048667</td>\n",
       "      <td>0.045425</td>\n",
       "      <td>0.040572</td>\n",
       "      <td>0.050953</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.042166</td>\n",
       "      <td>0.045230</td>\n",
       "      <td>0.046186</td>\n",
       "      <td>0.043899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.022983</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.027157</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>0.130637</td>\n",
       "      <td>0.110645</td>\n",
       "      <td>0.120394</td>\n",
       "      <td>0.122708</td>\n",
       "      <td>0.126439</td>\n",
       "      <td>0.110826</td>\n",
       "      <td>0.111382</td>\n",
       "      <td>0.126231</td>\n",
       "      <td>0.159306</td>\n",
       "      <td>0.115032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091067</td>\n",
       "      <td>0.087054</td>\n",
       "      <td>0.071257</td>\n",
       "      <td>0.082721</td>\n",
       "      <td>0.079852</td>\n",
       "      <td>0.079148</td>\n",
       "      <td>0.085198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>0.027519</td>\n",
       "      <td>0.042642</td>\n",
       "      <td>0.042732</td>\n",
       "      <td>0.042214</td>\n",
       "      <td>0.037931</td>\n",
       "      <td>0.038325</td>\n",
       "      <td>0.048778</td>\n",
       "      <td>0.059403</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.046506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.004079</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.007536</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>0.005234</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.004161</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035666</td>\n",
       "      <td>0.038966</td>\n",
       "      <td>0.042347</td>\n",
       "      <td>0.038167</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.037942</td>\n",
       "      <td>0.044095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>0.027533</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>0.027987</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.023503</td>\n",
       "      <td>0.025018</td>\n",
       "      <td>0.032983</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>0.026411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087155</td>\n",
       "      <td>0.089602</td>\n",
       "      <td>0.095711</td>\n",
       "      <td>0.101972</td>\n",
       "      <td>0.108628</td>\n",
       "      <td>0.086491</td>\n",
       "      <td>0.099172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2824 rows × 1603 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2000      2005      2010      2015      2020      2025      2030   \n",
       "0     0.021589  0.022946  0.020381  0.019762  0.022195  0.020017  0.020972  \\\n",
       "1     0.009301  0.008272  0.008337  0.007711  0.007782  0.007348  0.007808   \n",
       "2     0.025274  0.019463  0.019516  0.020692  0.018069  0.018431  0.023269   \n",
       "3     0.020099  0.020638  0.019760  0.018159  0.018335  0.022901  0.065560   \n",
       "4     0.008984  0.008043  0.013672  0.010317  0.009059  0.010720  0.013516   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2819  0.054429  0.048667  0.045425  0.040572  0.050953  0.044444  0.042166   \n",
       "2820  0.130637  0.110645  0.120394  0.122708  0.126439  0.110826  0.111382   \n",
       "2821  0.027519  0.042642  0.042732  0.042214  0.037931  0.038325  0.048778   \n",
       "2822  0.005901  0.004834  0.007536  0.004969  0.005234  0.004168  0.004910   \n",
       "2823  0.027533  0.024239  0.027987  0.025122  0.023503  0.025018  0.032983   \n",
       "\n",
       "          2035      2040      2045  ...      9965      9970      9975   \n",
       "0     0.023774  0.026180  0.022159  ...  0.033513  0.041287  0.036239  \\\n",
       "1     0.007108  0.006898  0.008332  ...  0.023975  0.026137  0.021943   \n",
       "2     0.020666  0.020079  0.020810  ...  0.025799  0.021951  0.029973   \n",
       "3     0.026733  0.020303  0.027320  ...  0.050939  0.053343  0.051328   \n",
       "4     0.008752  0.008767  0.007783  ...  0.158928  0.170674  0.173528   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2819  0.045230  0.046186  0.043899  ...  0.023866  0.023817  0.022983   \n",
       "2820  0.126231  0.159306  0.115032  ...  0.091067  0.087054  0.071257   \n",
       "2821  0.059403  0.038095  0.046506  ...  0.004087  0.005437  0.004977   \n",
       "2822  0.003472  0.004161  0.004788  ...  0.035666  0.038966  0.042347   \n",
       "2823  0.031111  0.024735  0.026411  ...  0.087155  0.089602  0.095711   \n",
       "\n",
       "          9980      9985      9990      9995  Oxacillin  Clindamycin   \n",
       "0     0.030211  0.035161  0.033998  0.038221        0.0          0.0  \\\n",
       "1     0.024854  0.029517  0.022439  0.025007        0.0          0.0   \n",
       "2     0.025818  0.029609  0.024297  0.027897        0.0          0.0   \n",
       "3     0.053429  0.050428  0.047393  0.054734        0.0          0.0   \n",
       "4     0.182519  0.194263  0.213917  0.213849        0.0          0.0   \n",
       "...        ...       ...       ...       ...        ...          ...   \n",
       "2819  0.019162  0.027157  0.020898  0.019744        0.0          1.0   \n",
       "2820  0.082721  0.079852  0.079148  0.085198        0.0          1.0   \n",
       "2821  0.004079  0.004673  0.006618  0.006707        0.0          0.0   \n",
       "2822  0.038167  0.031830  0.037942  0.044095        1.0          1.0   \n",
       "2823  0.101972  0.108628  0.086491  0.099172        0.0          0.0   \n",
       "\n",
       "      Fusidic acid  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  \n",
       "...            ...  \n",
       "2819           0.0  \n",
       "2820           0.0  \n",
       "2821           0.0  \n",
       "2822           0.0  \n",
       "2823           0.0  \n",
       "\n",
       "[2824 rows x 1603 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = \"data/processed/binned/train_s_aureus_driams_bin5.csv\"\n",
    "train_bac = pd.read_csv(train_file)\n",
    "train_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2005</th>\n",
       "      <th>2010</th>\n",
       "      <th>2015</th>\n",
       "      <th>2020</th>\n",
       "      <th>2025</th>\n",
       "      <th>2030</th>\n",
       "      <th>2035</th>\n",
       "      <th>2040</th>\n",
       "      <th>2045</th>\n",
       "      <th>...</th>\n",
       "      <th>9965</th>\n",
       "      <th>9970</th>\n",
       "      <th>9975</th>\n",
       "      <th>9980</th>\n",
       "      <th>9985</th>\n",
       "      <th>9990</th>\n",
       "      <th>9995</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041853</td>\n",
       "      <td>0.035847</td>\n",
       "      <td>0.044014</td>\n",
       "      <td>0.038222</td>\n",
       "      <td>0.035438</td>\n",
       "      <td>0.039194</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.036699</td>\n",
       "      <td>0.030814</td>\n",
       "      <td>0.039204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237908</td>\n",
       "      <td>0.260413</td>\n",
       "      <td>0.258291</td>\n",
       "      <td>0.247243</td>\n",
       "      <td>0.257622</td>\n",
       "      <td>0.248413</td>\n",
       "      <td>0.259787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033095</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>0.043297</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.041545</td>\n",
       "      <td>0.038463</td>\n",
       "      <td>0.054834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.031966</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>0.025964</td>\n",
       "      <td>0.027845</td>\n",
       "      <td>0.027336</td>\n",
       "      <td>0.026756</td>\n",
       "      <td>0.037775</td>\n",
       "      <td>0.029120</td>\n",
       "      <td>0.028391</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081617</td>\n",
       "      <td>0.074498</td>\n",
       "      <td>0.083699</td>\n",
       "      <td>0.075527</td>\n",
       "      <td>0.087347</td>\n",
       "      <td>0.084678</td>\n",
       "      <td>0.078530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014630</td>\n",
       "      <td>0.016686</td>\n",
       "      <td>0.017312</td>\n",
       "      <td>0.015131</td>\n",
       "      <td>0.015455</td>\n",
       "      <td>0.015299</td>\n",
       "      <td>0.016177</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>0.015993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006461</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.004908</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.005849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045526</td>\n",
       "      <td>0.030720</td>\n",
       "      <td>0.040927</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>0.033791</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.030569</td>\n",
       "      <td>0.028181</td>\n",
       "      <td>0.036521</td>\n",
       "      <td>0.027483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.026728</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>0.027527</td>\n",
       "      <td>0.028264</td>\n",
       "      <td>0.034543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.014471</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>0.011513</td>\n",
       "      <td>0.011093</td>\n",
       "      <td>0.012214</td>\n",
       "      <td>0.020299</td>\n",
       "      <td>0.011281</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046434</td>\n",
       "      <td>0.045882</td>\n",
       "      <td>0.059404</td>\n",
       "      <td>0.064590</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.053808</td>\n",
       "      <td>0.057879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.020435</td>\n",
       "      <td>0.016833</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.017066</td>\n",
       "      <td>0.019697</td>\n",
       "      <td>0.015848</td>\n",
       "      <td>0.016964</td>\n",
       "      <td>0.015639</td>\n",
       "      <td>0.018255</td>\n",
       "      <td>0.018938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051071</td>\n",
       "      <td>0.049248</td>\n",
       "      <td>0.050468</td>\n",
       "      <td>0.052308</td>\n",
       "      <td>0.050289</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.047683</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>0.053406</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>0.043005</td>\n",
       "      <td>0.044440</td>\n",
       "      <td>0.041648</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.049676</td>\n",
       "      <td>0.044953</td>\n",
       "      <td>0.041232</td>\n",
       "      <td>0.047865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098670</td>\n",
       "      <td>0.091627</td>\n",
       "      <td>0.092702</td>\n",
       "      <td>0.084402</td>\n",
       "      <td>0.089359</td>\n",
       "      <td>0.095558</td>\n",
       "      <td>0.089777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.013248</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.011026</td>\n",
       "      <td>0.009632</td>\n",
       "      <td>0.011725</td>\n",
       "      <td>0.010005</td>\n",
       "      <td>0.010293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051445</td>\n",
       "      <td>0.062624</td>\n",
       "      <td>0.046917</td>\n",
       "      <td>0.056096</td>\n",
       "      <td>0.057414</td>\n",
       "      <td>0.056392</td>\n",
       "      <td>0.063282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.024086</td>\n",
       "      <td>0.022550</td>\n",
       "      <td>0.022707</td>\n",
       "      <td>0.024213</td>\n",
       "      <td>0.025880</td>\n",
       "      <td>0.037046</td>\n",
       "      <td>0.027796</td>\n",
       "      <td>0.023253</td>\n",
       "      <td>0.031724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054939</td>\n",
       "      <td>0.059052</td>\n",
       "      <td>0.062933</td>\n",
       "      <td>0.055132</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.049864</td>\n",
       "      <td>0.048264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>707 rows × 1603 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2000      2005      2010      2015      2020      2025      2030   \n",
       "0    0.041853  0.035847  0.044014  0.038222  0.035438  0.039194  0.045455  \\\n",
       "1    0.001734  0.002046  0.002741  0.001489  0.001695  0.002306  0.003009   \n",
       "2    0.031966  0.029968  0.025964  0.027845  0.027336  0.026756  0.037775   \n",
       "3    0.014630  0.016686  0.017312  0.015131  0.015455  0.015299  0.016177   \n",
       "4    0.045526  0.030720  0.040927  0.039286  0.033791  0.033557  0.030569   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "702  0.014471  0.010937  0.009533  0.011513  0.011093  0.012214  0.020299   \n",
       "703  0.020435  0.016833  0.015744  0.017066  0.019697  0.015848  0.016964   \n",
       "704  0.053406  0.045796  0.043005  0.044440  0.041648  0.043400  0.049676   \n",
       "705  0.013248  0.012038  0.011121  0.010921  0.011928  0.011026  0.009632   \n",
       "706  0.027775  0.024086  0.022550  0.022707  0.024213  0.025880  0.037046   \n",
       "\n",
       "         2035      2040      2045  ...      9965      9970      9975   \n",
       "0    0.036699  0.030814  0.039204  ...  0.237908  0.260413  0.258291  \\\n",
       "1    0.002233  0.001295  0.000921  ...  0.033095  0.032597  0.043297   \n",
       "2    0.029120  0.028391  0.032304  ...  0.081617  0.074498  0.083699   \n",
       "3    0.016479  0.014907  0.015993  ...  0.006461  0.006438  0.004570   \n",
       "4    0.028181  0.036521  0.027483  ...  0.025843  0.026728  0.027100   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "702  0.011281  0.012552  0.014815  ...  0.046434  0.045882  0.059404   \n",
       "703  0.015639  0.018255  0.018938  ...  0.051071  0.049248  0.050468   \n",
       "704  0.044953  0.041232  0.047865  ...  0.098670  0.091627  0.092702   \n",
       "705  0.011725  0.010005  0.010293  ...  0.051445  0.062624  0.046917   \n",
       "706  0.027796  0.023253  0.031724  ...  0.054939  0.059052  0.062933   \n",
       "\n",
       "         9980      9985      9990      9995  Oxacillin  Clindamycin   \n",
       "0    0.247243  0.257622  0.248413  0.259787        0.0          0.0  \\\n",
       "1    0.034783  0.041545  0.038463  0.054834        0.0          0.0   \n",
       "2    0.075527  0.087347  0.084678  0.078530        0.0          1.0   \n",
       "3    0.004908  0.005598  0.004488  0.005849        0.0          0.0   \n",
       "4    0.032183  0.027527  0.028264  0.034543        0.0          0.0   \n",
       "..        ...       ...       ...       ...        ...          ...   \n",
       "702  0.064590  0.060467  0.053808  0.057879        0.0          0.0   \n",
       "703  0.052308  0.050289  0.051020  0.047683        1.0          0.0   \n",
       "704  0.084402  0.089359  0.095558  0.089777        0.0          0.0   \n",
       "705  0.056096  0.057414  0.056392  0.063282        1.0          0.0   \n",
       "706  0.055132  0.063636  0.049864  0.048264        0.0          0.0   \n",
       "\n",
       "     Fusidic acid  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             1.0  \n",
       "4             0.0  \n",
       "..            ...  \n",
       "702           0.0  \n",
       "703           0.0  \n",
       "704           1.0  \n",
       "705           0.0  \n",
       "706           0.0  \n",
       "\n",
       "[707 rows x 1603 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = \"data/processed/binned/test_s_aureus_driams_bin5.csv\"\n",
    "test_bac = pd.read_csv(test_file)\n",
    "test_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rXEshQwKQuzR",
    "outputId": "32756288-dd35-49f1-be9b-34bfe433baf7",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_bac[train_bac.columns.drop(list(train_bac.filter(regex='[^0-9]')))]\n",
    "test_x = test_bac[test_bac.columns.drop(list(test_bac.filter(regex='[^0-9]')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GiCCaGOEQuzS",
    "outputId": "bdc15429-4a19-4332-d59f-dd1c0ce37d88",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "antibiotics = train_bac.columns.drop(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_bac[antibiotics]\n",
    "test_y = test_bac[antibiotics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multilabel_f1_wrapper(true, pred, average=\"weighted\"):\n",
    "    if isinstance(true, list):\n",
    "        true = np.array(true)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        true = true.to_numpy()\n",
    "    if isinstance(pred, list):\n",
    "        pred = np.array(pred)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        pred = pred.to_numpy()\n",
    "    column = 0\n",
    "    total = 0\n",
    "    while column < true[0].size:\n",
    "        total+=f1_score(true[:, column], pred[:, column], average=average)\n",
    "        column+=1\n",
    "    return total/(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def report(true, pred):\n",
    "        \n",
    "    hl = hamming_loss(true, pred)\n",
    "    f1w = multilabel_f1_wrapper(true, pred, \"weighted\")\n",
    "    acc = accuracy_score(true, pred)\n",
    "    \n",
    "    f1u = multilabel_f1_wrapper(true, pred, \"macro\")\n",
    "    f1su = f1_score(true, pred, average=\"macro\")\n",
    "    f1sw = f1_score(true, pred, average=\"weighted\")\n",
    "\n",
    "    \n",
    "    print(\"Main metrics:\")\n",
    "    print(\" Hamming Loss:\", hl)\n",
    "    print(\" Accuracy:\", acc)\n",
    "    print(\" F1 Score (Weighted):\", f1w)\n",
    "    print(\"================================================\")\n",
    "    print(\"Other metrics:\")\n",
    "    print(\" F1 Score (Unweighted):\", f1u)\n",
    "    print(\" F1 Score (sklearn Unweighted):\", f1su)\n",
    "    print(\" F1 Score (sklearn Weighted):\", f1sw)\n",
    "    return hl, acc, f1w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-gP_gv8QuzZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eocque/bac/venv/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=ClassifierChain(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                                     booster=None,\n",
       "                                                                     callbacks=None,\n",
       "                                                                     colsample_bylevel=None,\n",
       "                                                                     colsample_bynode=None,\n",
       "                                                                     colsample_bytree=None,\n",
       "                                                                     early_stopping_rounds=None,\n",
       "                                                                     enable_categorical=False,\n",
       "                                                                     eval_metric=None,\n",
       "                                                                     feature_types=None,\n",
       "                                                                     gamma=None,\n",
       "                                                                     gpu_id=None,\n",
       "                                                                     grow_policy=None,\n",
       "                                                                     importance_type=None,\n",
       "                                                                     interaction_cons...\n",
       "                             &#x27;base_estimator__objective&#x27;: Categorical(categories=(&#x27;binary:logistic&#x27;,), prior=None),\n",
       "                             &#x27;base_estimator__scale_pos_weight&#x27;: Real(low=1e-06, high=10, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__subsample&#x27;: Real(low=1e-06, high=1, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__tree_method&#x27;: Categorical(categories=(&#x27;exact&#x27;, &#x27;approx&#x27;, &#x27;hist&#x27;), prior=None)},\n",
       "              verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=ClassifierChain(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                                     booster=None,\n",
       "                                                                     callbacks=None,\n",
       "                                                                     colsample_bylevel=None,\n",
       "                                                                     colsample_bynode=None,\n",
       "                                                                     colsample_bytree=None,\n",
       "                                                                     early_stopping_rounds=None,\n",
       "                                                                     enable_categorical=False,\n",
       "                                                                     eval_metric=None,\n",
       "                                                                     feature_types=None,\n",
       "                                                                     gamma=None,\n",
       "                                                                     gpu_id=None,\n",
       "                                                                     grow_policy=None,\n",
       "                                                                     importance_type=None,\n",
       "                                                                     interaction_cons...\n",
       "                             &#x27;base_estimator__objective&#x27;: Categorical(categories=(&#x27;binary:logistic&#x27;,), prior=None),\n",
       "                             &#x27;base_estimator__scale_pos_weight&#x27;: Real(low=1e-06, high=10, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__subsample&#x27;: Real(low=1e-06, high=1, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__tree_method&#x27;: Categorical(categories=(&#x27;exact&#x27;, &#x27;approx&#x27;, &#x27;hist&#x27;), prior=None)},\n",
       "              verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(base_estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                             callbacks=None,\n",
       "                                             colsample_bylevel=None,\n",
       "                                             colsample_bynode=None,\n",
       "                                             colsample_bytree=None,\n",
       "                                             early_stopping_rounds=None,\n",
       "                                             enable_categorical=False,\n",
       "                                             eval_metric=None,\n",
       "                                             feature_types=None, gamma=None,\n",
       "                                             gpu_id=None, grow_policy=None,\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate=None, max_bin=None,\n",
       "                                             max_cat_threshold=None,\n",
       "                                             max_cat_to_onehot=None,\n",
       "                                             max_delta_step=None,\n",
       "                                             max_depth=None, max_leaves=None,\n",
       "                                             min_child_weight=None, missing=nan,\n",
       "                                             monotone_constraints=None,\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             num_parallel_tree=None,\n",
       "                                             predictor=None, random_state=None, ...),\n",
       "                random_state=0)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=ClassifierChain(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                                     booster=None,\n",
       "                                                                     callbacks=None,\n",
       "                                                                     colsample_bylevel=None,\n",
       "                                                                     colsample_bynode=None,\n",
       "                                                                     colsample_bytree=None,\n",
       "                                                                     early_stopping_rounds=None,\n",
       "                                                                     enable_categorical=False,\n",
       "                                                                     eval_metric=None,\n",
       "                                                                     feature_types=None,\n",
       "                                                                     gamma=None,\n",
       "                                                                     gpu_id=None,\n",
       "                                                                     grow_policy=None,\n",
       "                                                                     importance_type=None,\n",
       "                                                                     interaction_cons...\n",
       "                             'base_estimator__objective': Categorical(categories=('binary:logistic',), prior=None),\n",
       "                             'base_estimator__scale_pos_weight': Real(low=1e-06, high=10, prior='log-uniform', transform='normalize'),\n",
       "                             'base_estimator__subsample': Real(low=1e-06, high=1, prior='log-uniform', transform='normalize'),\n",
       "                             'base_estimator__tree_method': Categorical(categories=('exact', 'approx', 'hist'), prior=None)},\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesopt = BayesSearchCV(\n",
    "    ClassifierChain(xgb.XGBClassifier(), random_state=0),\n",
    "    {\n",
    "        \"base_estimator__objective\": Categorical([\"binary:logistic\"]),\n",
    "        \"base_estimator__max_depth\": Integer(1, 10),\n",
    "        \"base_estimator__min_child_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__max_delta_step\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__subsample\": Real(1e-6, 1, prior=\"log-uniform\"),\n",
    "        \"base_estimator__tree_method\": Categorical([\"exact\", \"approx\", \"hist\"]),\n",
    "        \"base_estimator__scale_pos_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__gamma\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__eta\": Real(1e-6, 1, prior=\"log-uniform\")\n",
    "    },\n",
    "    n_iter=250,\n",
    "    cv=5,\n",
    "    random_state=0,\n",
    "    n_jobs=10,\n",
    "    n_points=2,\n",
    "    scoring=make_scorer(multilabel_f1_wrapper),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "bayesopt.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 114\n",
      "Split scores:\n",
      " 0 0.8667538349934789\n",
      " 1 0.8708601358644437\n",
      " 2 0.8679108217754337\n",
      " 3 0.8597316735908994\n",
      " 4 0.8702938142457953\n",
      "Mean score: 0.8671100560940103\n",
      "Best parameter combination found: OrderedDict([('base_estimator__eta', 0.10727437228720393), ('base_estimator__gamma', 1e-06), ('base_estimator__max_delta_step', 10.0), ('base_estimator__max_depth', 10), ('base_estimator__min_child_weight', 4.561345698036995e-06), ('base_estimator__objective', 'binary:logistic'), ('base_estimator__scale_pos_weight', 10.0), ('base_estimator__subsample', 1.0), ('base_estimator__tree_method', 'approx')])\n"
     ]
    }
   ],
   "source": [
    "best_iteration = 0\n",
    "for i in range(0, 250):\n",
    "    if bayesopt.cv_results_[\"mean_test_score\"][i] == bayesopt.best_score_:\n",
    "        best_iteration = i\n",
    "print(\"Best iteration:\", best_iteration)\n",
    "print(\"Split scores:\")\n",
    "for i in range(0, 5):\n",
    "    print(\"\", i, bayesopt.cv_results_[\"split\"+str(i)+\"_test_score\"][best_iteration])\n",
    "    \n",
    "print(\"Mean score:\", bayesopt.best_score_)\n",
    "print(\"Best parameter combination found:\", bayesopt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"xgb_s_aureus_raw_bin5.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_s_aureus_raw_bin5.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(bayesopt.best_estimator_, model_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main metrics:\n",
      " Hamming Loss: 0.1074964639321075\n",
      " Accuracy: 0.7355021216407355\n",
      " F1 Score (Weighted): 0.8706902397019406\n",
      "================================================\n",
      "Other metrics:\n",
      " F1 Score (Unweighted): 0.6404512018382215\n",
      " F1 Score (sklearn Unweighted): 0.34152383904127287\n",
      " F1 Score (sklearn Weighted): 0.42828747874570294\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y) \n",
    "pred = model.predict(test_x)\n",
    "model_hl, model_acc, model_f1 = report(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, len(antibiotics), figsize=(len(antibiotics)*5, 5))\n",
    "fig.supxlabel(\"Predicted Label\")\n",
    "fig.supylabel(\"True Label\")\n",
    "\n",
    "cm_svm_c = multilabel_confusion_matrix(test_y, (pred > 0.5))\n",
    "\n",
    "for i in range(len(antibiotics)):\n",
    "  sns.heatmap(ax=axes[i], data=cm_svm_c[i], annot=True, fmt='d', cbar=None, cmap=\"Blues\", xticklabels=[\"S\", \"R\"], yticklabels=[\"S\", \"R\"]).set(title=antibiotics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.29187799e-01, 4.34125096e-01, 2.87377508e-03],\n",
       "       [8.37214384e-03, 2.89933514e-02, 1.22052256e-03],\n",
       "       [2.32664542e-03, 7.58598372e-02, 7.67447054e-04],\n",
       "       ...,\n",
       "       [3.50867242e-01, 2.60172822e-02, 3.55168199e-03],\n",
       "       [1.22982889e-01, 1.40133146e-02, 7.36415898e-03],\n",
       "       [1.89003889e-02, 2.05379631e-02, 3.29515315e-04]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = model.predict_proba(test_x)\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for antibiotic Oxacillin\n",
      " Mean TP: 0.8978810004698925\n",
      " Mean TN: 0.0492518988245311\n",
      " Mean FP: 0.7375905105942174\n",
      " Mean FN: 0.11671737535471038\n",
      "Results for antibiotic Clindamycin\n",
      " Mean TP: 0.781771042767693\n",
      " Mean TN: 0.061149276560567734\n",
      " Mean FP: 0.7092353217303753\n",
      " Mean FN: 0.09189601412234719\n",
      "Results for antibiotic Fusidic acid\n",
      " Mean TP: 0.6548689802487692\n",
      " Mean TN: 0.012865500991844756\n",
      " Mean FP: 0.7688330411911011\n",
      " Mean FN: 0.040968368606713955\n"
     ]
    }
   ],
   "source": [
    "for antibiotic in range(len(antibiotics)):\n",
    "    count_tp = 0\n",
    "    count_tn = 0\n",
    "    count_fp = 0\n",
    "    count_fn = 0\n",
    "    sum_tp = 0\n",
    "    sum_tn = 0\n",
    "    sum_fp = 0\n",
    "    sum_fn = 0\n",
    "    for i in range(len(proba[:, antibiotic])):\n",
    "        disc_pred = int(proba[i, antibiotic] > 0.5)\n",
    "        if disc_pred == 1:\n",
    "            if disc_pred == test_y.iloc[i, antibiotic]:\n",
    "                count_tp += 1\n",
    "                sum_tp += proba[i, antibiotic]\n",
    "            else:\n",
    "                count_fp += 1\n",
    "                sum_fp += proba[i, antibiotic]\n",
    "        else:\n",
    "            if disc_pred == test_y.iloc[i, antibiotic]:\n",
    "                count_tn += 1\n",
    "                sum_tn += proba[i, antibiotic]\n",
    "            else:\n",
    "                count_fn += 1\n",
    "                sum_fn += proba[i, antibiotic]\n",
    "    print(\"Results for antibiotic\", antibiotics[antibiotic])\n",
    "    if count_tp == 0:\n",
    "        print(\" Mean TP: None\")\n",
    "    else: \n",
    "        print(\" Mean TP:\", sum_tp/count_tp)\n",
    "    if count_tn == 0:\n",
    "        print(\" Mean TN: None\")\n",
    "    else: \n",
    "        print(\" Mean TN:\", sum_tn/count_tn)\n",
    "    if count_fp == 0:\n",
    "        print(\" Mean FP: None\")\n",
    "    else: \n",
    "        print(\" Mean FP:\", sum_fp/count_fp)\n",
    "    if count_fn == 0:\n",
    "        print(\" Mean FN: None\")\n",
    "    else: \n",
    "        print(\" Mean FN:\", sum_fn/count_fn)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
