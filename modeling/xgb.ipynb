{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/castudil/bacteria-multi-label/blob/main/multilabel_bac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKUeIuZcpHUl"
   },
   "source": [
    "Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bzVprbfpWSLa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import (f1_score, multilabel_confusion_matrix,\n",
    "                             accuracy_score, hamming_loss, jaccard_score, make_scorer)\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from joblib import dump, load\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "nQsWnulDWdDD",
    "outputId": "6f97687c-b560-4e34-fd8b-9c3ef7aeb0c7",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.016147</td>\n",
       "      <td>0.016983</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.019405</td>\n",
       "      <td>0.023356</td>\n",
       "      <td>0.026224</td>\n",
       "      <td>0.026569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037966</td>\n",
       "      <td>0.030364</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>0.040851</td>\n",
       "      <td>0.034176</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.025638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009001</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014496</td>\n",
       "      <td>0.024966</td>\n",
       "      <td>0.027437</td>\n",
       "      <td>0.026541</td>\n",
       "      <td>0.022940</td>\n",
       "      <td>0.020572</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>0.020910</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.021436</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.020229</td>\n",
       "      <td>0.018818</td>\n",
       "      <td>0.018637</td>\n",
       "      <td>0.018815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024620</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.026715</td>\n",
       "      <td>0.032045</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>0.029085</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.016407</td>\n",
       "      <td>0.018011</td>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.019116</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051312</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.049338</td>\n",
       "      <td>0.055039</td>\n",
       "      <td>0.054541</td>\n",
       "      <td>0.058643</td>\n",
       "      <td>0.058919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236769</td>\n",
       "      <td>0.217499</td>\n",
       "      <td>0.187244</td>\n",
       "      <td>0.216243</td>\n",
       "      <td>0.221910</td>\n",
       "      <td>0.226531</td>\n",
       "      <td>0.221965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>0.056616</td>\n",
       "      <td>0.039011</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>0.048517</td>\n",
       "      <td>0.050865</td>\n",
       "      <td>0.047771</td>\n",
       "      <td>0.049312</td>\n",
       "      <td>0.048257</td>\n",
       "      <td>0.049417</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021169</td>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.021037</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>0.125837</td>\n",
       "      <td>0.107712</td>\n",
       "      <td>0.109186</td>\n",
       "      <td>0.107613</td>\n",
       "      <td>0.109855</td>\n",
       "      <td>0.105060</td>\n",
       "      <td>0.099640</td>\n",
       "      <td>0.104169</td>\n",
       "      <td>0.120303</td>\n",
       "      <td>0.125067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082375</td>\n",
       "      <td>0.083446</td>\n",
       "      <td>0.096510</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.092228</td>\n",
       "      <td>0.085599</td>\n",
       "      <td>0.042142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035603</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>0.042372</td>\n",
       "      <td>0.046666</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.043914</td>\n",
       "      <td>0.039875</td>\n",
       "      <td>0.037170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.006124</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049241</td>\n",
       "      <td>0.039586</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.039139</td>\n",
       "      <td>0.046816</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.022810</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>0.026694</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.025140</td>\n",
       "      <td>0.026609</td>\n",
       "      <td>0.024203</td>\n",
       "      <td>0.020953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090198</td>\n",
       "      <td>0.101889</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>0.122065</td>\n",
       "      <td>0.095740</td>\n",
       "      <td>0.082289</td>\n",
       "      <td>0.081714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2824 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2000      2001      2002      2003      2004      2005      2006   \n",
       "0     0.018721  0.016147  0.016983  0.021218  0.020846  0.019784  0.019405  \\\n",
       "1     0.009001  0.007475  0.006874  0.008575  0.009539  0.007894  0.008314   \n",
       "2     0.022354  0.020220  0.020910  0.024631  0.021436  0.021197  0.020229   \n",
       "3     0.017619  0.016073  0.016407  0.018011  0.019364  0.018950  0.017607   \n",
       "4     0.008264  0.008229  0.006753  0.006657  0.010107  0.007039  0.008250   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2819  0.056616  0.039011  0.040380  0.048517  0.050865  0.047771  0.049312   \n",
       "2820  0.125837  0.107712  0.109186  0.107613  0.109855  0.105060  0.099640   \n",
       "2821  0.000000  0.000000  0.035603  0.039994  0.042372  0.046666  0.045781   \n",
       "2822  0.005443  0.005998  0.003670  0.005588  0.006124  0.005019  0.004853   \n",
       "2823  0.026184  0.022810  0.023200  0.020464  0.026694  0.024624  0.025140   \n",
       "\n",
       "          2007      2008      2009  ...      9993      9994      9995   \n",
       "0     0.023356  0.026224  0.026569  ...  0.037966  0.030364  0.037545  \\\n",
       "1     0.008013  0.008664  0.008923  ...  0.014496  0.024966  0.027437   \n",
       "2     0.018818  0.018637  0.018815  ...  0.024620  0.022942  0.026715   \n",
       "3     0.019116  0.023623  0.024492  ...  0.051312  0.047458  0.049338   \n",
       "4     0.010670  0.008134  0.006513  ...  0.236769  0.217499  0.187244   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2819  0.048257  0.049417  0.049000  ...  0.021169  0.023617  0.033694   \n",
       "2820  0.104169  0.120303  0.125067  ...  0.082375  0.083446  0.096510   \n",
       "2821  0.043914  0.039875  0.037170  ...  0.006903  0.008322  0.011071   \n",
       "2822  0.005400  0.004169  0.005151  ...  0.049241  0.039586  0.050542   \n",
       "2823  0.026609  0.024203  0.020953  ...  0.090198  0.101889  0.105174   \n",
       "\n",
       "          9996      9997      9998      9999  Oxacillin  Clindamycin   \n",
       "0     0.040851  0.034176  0.046110  0.025638        0.0          0.0  \\\n",
       "1     0.026541  0.022940  0.020572  0.032504        0.0          0.0   \n",
       "2     0.032045  0.030431  0.029085  0.013117        0.0          0.0   \n",
       "3     0.055039  0.054541  0.058643  0.058919        0.0          0.0   \n",
       "4     0.216243  0.221910  0.226531  0.221965        0.0          0.0   \n",
       "...        ...       ...       ...       ...        ...          ...   \n",
       "2819  0.021037  0.018727  0.010641  0.009238        0.0          1.0   \n",
       "2820  0.084883  0.092228  0.085599  0.042142        0.0          1.0   \n",
       "2821  0.010274  0.004682  0.003547  0.001744        0.0          0.0   \n",
       "2822  0.039139  0.046816  0.043036  0.037402        1.0          1.0   \n",
       "2823  0.122065  0.095740  0.082289  0.081714        0.0          0.0   \n",
       "\n",
       "      Fusidic acid  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  \n",
       "...            ...  \n",
       "2819           0.0  \n",
       "2820           0.0  \n",
       "2821           0.0  \n",
       "2822           0.0  \n",
       "2823           0.0  \n",
       "\n",
       "[2824 rows x 8003 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = \"data/processed/raw/train_s_aureus_driams.csv\"\n",
    "train_bac = pd.read_csv(train_file)\n",
    "train_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044453</td>\n",
       "      <td>0.032486</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>0.034223</td>\n",
       "      <td>0.037528</td>\n",
       "      <td>0.039503</td>\n",
       "      <td>0.031378</td>\n",
       "      <td>0.035506</td>\n",
       "      <td>0.037688</td>\n",
       "      <td>0.035658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247814</td>\n",
       "      <td>0.263833</td>\n",
       "      <td>0.279904</td>\n",
       "      <td>0.264432</td>\n",
       "      <td>0.241573</td>\n",
       "      <td>0.266020</td>\n",
       "      <td>0.231517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004318</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033364</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>0.066426</td>\n",
       "      <td>0.057485</td>\n",
       "      <td>0.052903</td>\n",
       "      <td>0.050839</td>\n",
       "      <td>0.036631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026459</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.028609</td>\n",
       "      <td>0.031314</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.028051</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>0.028978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.094103</td>\n",
       "      <td>0.080969</td>\n",
       "      <td>0.073970</td>\n",
       "      <td>0.069047</td>\n",
       "      <td>0.070988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>0.014582</td>\n",
       "      <td>0.015084</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.018272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.007583</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060499</td>\n",
       "      <td>0.043034</td>\n",
       "      <td>0.030296</td>\n",
       "      <td>0.032635</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>0.032762</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>0.030382</td>\n",
       "      <td>0.030233</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029452</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>0.039230</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>0.037219</td>\n",
       "      <td>0.022227</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.013092</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.009902</td>\n",
       "      <td>0.013513</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.010994</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050851</td>\n",
       "      <td>0.046334</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.066948</td>\n",
       "      <td>0.061007</td>\n",
       "      <td>0.056178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.021222</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>0.015512</td>\n",
       "      <td>0.017995</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.019427</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047285</td>\n",
       "      <td>0.056455</td>\n",
       "      <td>0.058002</td>\n",
       "      <td>0.055528</td>\n",
       "      <td>0.043539</td>\n",
       "      <td>0.039962</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>0.045613</td>\n",
       "      <td>0.041040</td>\n",
       "      <td>0.046177</td>\n",
       "      <td>0.050216</td>\n",
       "      <td>0.046525</td>\n",
       "      <td>0.048843</td>\n",
       "      <td>0.045920</td>\n",
       "      <td>0.044283</td>\n",
       "      <td>0.043983</td>\n",
       "      <td>0.046330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104234</td>\n",
       "      <td>0.094692</td>\n",
       "      <td>0.090012</td>\n",
       "      <td>0.096624</td>\n",
       "      <td>0.092228</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.043682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.015193</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>0.010474</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.014157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051772</td>\n",
       "      <td>0.058255</td>\n",
       "      <td>0.074609</td>\n",
       "      <td>0.068249</td>\n",
       "      <td>0.049157</td>\n",
       "      <td>0.070229</td>\n",
       "      <td>0.043615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.022209</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.025181</td>\n",
       "      <td>0.025069</td>\n",
       "      <td>0.025761</td>\n",
       "      <td>0.022690</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052692</td>\n",
       "      <td>0.061853</td>\n",
       "      <td>0.048375</td>\n",
       "      <td>0.060665</td>\n",
       "      <td>0.043305</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>0.048284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>707 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2000      2001      2002      2003      2004      2005      2006   \n",
       "0    0.044453  0.032486  0.032540  0.034223  0.037528  0.039503  0.031378  \\\n",
       "1    0.004318  0.001881  0.001274  0.000902  0.000892  0.000049  0.002188   \n",
       "2    0.026184  0.026459  0.025393  0.028609  0.031314  0.031739  0.033337   \n",
       "3    0.000000  0.015010  0.017782  0.014582  0.015084  0.018046  0.014461   \n",
       "4    0.060499  0.043034  0.030296  0.032635  0.031272  0.032762  0.031154   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "702  0.013092  0.011730  0.009902  0.013513  0.015317  0.011370  0.013338   \n",
       "703  0.021222  0.015896  0.015512  0.017995  0.018663  0.019427  0.018667   \n",
       "704  0.045613  0.041040  0.046177  0.050216  0.046525  0.048843  0.045920   \n",
       "705  0.015193  0.011922  0.010877  0.009975  0.010474  0.012171  0.010417   \n",
       "706  0.025453  0.022209  0.022442  0.025181  0.025069  0.025761  0.022690   \n",
       "\n",
       "         2007      2008      2009  ...      9993      9994      9995   \n",
       "0    0.035506  0.037688  0.035658  ...  0.247814  0.263833  0.279904  \\\n",
       "1    0.002001  0.003081  0.003384  ...  0.033364  0.042735  0.066426   \n",
       "2    0.028051  0.028047  0.028978  ...  0.086746  0.087719  0.094103   \n",
       "3    0.014400  0.018769  0.018272  ...  0.005062  0.006748  0.004573   \n",
       "4    0.030382  0.030233  0.029438  ...  0.029452  0.033288  0.039230   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "702  0.010117  0.010994  0.009222  ...  0.050851  0.046334  0.042840   \n",
       "703  0.014589  0.016765  0.015071  ...  0.047285  0.056455  0.058002   \n",
       "704  0.044283  0.043983  0.046330  ...  0.104234  0.094692  0.090012   \n",
       "705  0.010783  0.013170  0.014157  ...  0.051772  0.058255  0.074609   \n",
       "706  0.024250  0.022596  0.025572  ...  0.052692  0.061853  0.048375   \n",
       "\n",
       "         9996      9997      9998      9999  Oxacillin  Clindamycin   \n",
       "0    0.264432  0.241573  0.266020  0.231517        0.0          0.0  \\\n",
       "1    0.057485  0.052903  0.050839  0.036631        0.0          0.0   \n",
       "2    0.080969  0.073970  0.069047  0.070988        0.0          1.0   \n",
       "3    0.007583  0.008427  0.004729  0.002394        0.0          0.0   \n",
       "4    0.046477  0.037219  0.022227  0.019920        0.0          0.0   \n",
       "..        ...       ...       ...       ...        ...          ...   \n",
       "702  0.061644  0.066948  0.061007  0.056178        0.0          0.0   \n",
       "703  0.055528  0.043539  0.039962  0.034761        1.0          0.0   \n",
       "704  0.096624  0.092228  0.100024  0.043682        0.0          0.0   \n",
       "705  0.068249  0.049157  0.070229  0.043615        1.0          0.0   \n",
       "706  0.060665  0.043305  0.041617  0.048284        0.0          0.0   \n",
       "\n",
       "     Fusidic acid  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             1.0  \n",
       "4             0.0  \n",
       "..            ...  \n",
       "702           0.0  \n",
       "703           0.0  \n",
       "704           1.0  \n",
       "705           0.0  \n",
       "706           0.0  \n",
       "\n",
       "[707 rows x 8003 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = \"data/processed/raw/test_s_aureus_driams.csv\"\n",
    "test_bac = pd.read_csv(test_file)\n",
    "test_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rXEshQwKQuzR",
    "outputId": "32756288-dd35-49f1-be9b-34bfe433baf7",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_bac[train_bac.columns.drop(list(train_bac.filter(regex='[^0-9]')))]\n",
    "test_x = test_bac[test_bac.columns.drop(list(test_bac.filter(regex='[^0-9]')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GiCCaGOEQuzS",
    "outputId": "bdc15429-4a19-4332-d59f-dd1c0ce37d88",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "antibiotics = train_bac.columns.drop(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_bac[antibiotics]\n",
    "test_y = test_bac[antibiotics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multilabel_f1_wrapper(true, pred, average=\"weighted\"):\n",
    "    if isinstance(true, list):\n",
    "        true = np.array(true)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        true = true.to_numpy()\n",
    "    if isinstance(pred, list):\n",
    "        pred = np.array(pred)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        pred = pred.to_numpy()\n",
    "    column = 0\n",
    "    total = 0\n",
    "    while column < true[0].size:\n",
    "        total+=f1_score(true[:, column], pred[:, column], average=average)\n",
    "        column+=1\n",
    "    return total/(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def report(true, pred):\n",
    "        \n",
    "    hl = hamming_loss(true, pred)\n",
    "    f1w = multilabel_f1_wrapper(true, pred, \"weighted\")\n",
    "    acc = accuracy_score(true, pred)\n",
    "    \n",
    "    f1u = multilabel_f1_wrapper(true, pred, \"macro\")\n",
    "    f1su = f1_score(true, pred, average=\"macro\")\n",
    "    f1sw = f1_score(true, pred, average=\"weighted\")\n",
    "\n",
    "    \n",
    "    print(\"Main metrics:\")\n",
    "    print(\" Hamming Loss:\", hl)\n",
    "    print(\" Accuracy:\", acc)\n",
    "    print(\" F1 Score (Weighted):\", f1w)\n",
    "    print(\"================================================\")\n",
    "    print(\"Other metrics:\")\n",
    "    print(\" F1 Score (Unweighted):\", f1u)\n",
    "    print(\" F1 Score (sklearn Unweighted):\", f1su)\n",
    "    print(\" F1 Score (sklearn Weighted):\", f1sw)\n",
    "    return hl, acc, f1w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-gP_gv8QuzZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=ClassifierChain(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                                     booster=None,\n",
       "                                                                     callbacks=None,\n",
       "                                                                     colsample_bylevel=None,\n",
       "                                                                     colsample_bynode=None,\n",
       "                                                                     colsample_bytree=None,\n",
       "                                                                     early_stopping_rounds=None,\n",
       "                                                                     enable_categorical=False,\n",
       "                                                                     eval_metric=None,\n",
       "                                                                     feature_types=None,\n",
       "                                                                     gamma=None,\n",
       "                                                                     gpu_id=None,\n",
       "                                                                     grow_policy=None,\n",
       "                                                                     importance_type=None,\n",
       "                                                                     interaction_cons...\n",
       "                             &#x27;base_estimator__objective&#x27;: Categorical(categories=(&#x27;binary:logistic&#x27;,), prior=None),\n",
       "                             &#x27;base_estimator__scale_pos_weight&#x27;: Real(low=1e-06, high=10, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__subsample&#x27;: Real(low=1e-06, high=1, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__tree_method&#x27;: Categorical(categories=(&#x27;exact&#x27;, &#x27;approx&#x27;, &#x27;hist&#x27;), prior=None)},\n",
       "              verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=ClassifierChain(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                                     booster=None,\n",
       "                                                                     callbacks=None,\n",
       "                                                                     colsample_bylevel=None,\n",
       "                                                                     colsample_bynode=None,\n",
       "                                                                     colsample_bytree=None,\n",
       "                                                                     early_stopping_rounds=None,\n",
       "                                                                     enable_categorical=False,\n",
       "                                                                     eval_metric=None,\n",
       "                                                                     feature_types=None,\n",
       "                                                                     gamma=None,\n",
       "                                                                     gpu_id=None,\n",
       "                                                                     grow_policy=None,\n",
       "                                                                     importance_type=None,\n",
       "                                                                     interaction_cons...\n",
       "                             &#x27;base_estimator__objective&#x27;: Categorical(categories=(&#x27;binary:logistic&#x27;,), prior=None),\n",
       "                             &#x27;base_estimator__scale_pos_weight&#x27;: Real(low=1e-06, high=10, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__subsample&#x27;: Real(low=1e-06, high=1, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__tree_method&#x27;: Categorical(categories=(&#x27;exact&#x27;, &#x27;approx&#x27;, &#x27;hist&#x27;), prior=None)},\n",
       "              verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(base_estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                             callbacks=None,\n",
       "                                             colsample_bylevel=None,\n",
       "                                             colsample_bynode=None,\n",
       "                                             colsample_bytree=None,\n",
       "                                             early_stopping_rounds=None,\n",
       "                                             enable_categorical=False,\n",
       "                                             eval_metric=None,\n",
       "                                             feature_types=None, gamma=None,\n",
       "                                             gpu_id=None, grow_policy=None,\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate=None, max_bin=None,\n",
       "                                             max_cat_threshold=None,\n",
       "                                             max_cat_to_onehot=None,\n",
       "                                             max_delta_step=None,\n",
       "                                             max_depth=None, max_leaves=None,\n",
       "                                             min_child_weight=None, missing=nan,\n",
       "                                             monotone_constraints=None,\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             num_parallel_tree=None,\n",
       "                                             predictor=None, random_state=None, ...),\n",
       "                random_state=0)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=ClassifierChain(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                                     booster=None,\n",
       "                                                                     callbacks=None,\n",
       "                                                                     colsample_bylevel=None,\n",
       "                                                                     colsample_bynode=None,\n",
       "                                                                     colsample_bytree=None,\n",
       "                                                                     early_stopping_rounds=None,\n",
       "                                                                     enable_categorical=False,\n",
       "                                                                     eval_metric=None,\n",
       "                                                                     feature_types=None,\n",
       "                                                                     gamma=None,\n",
       "                                                                     gpu_id=None,\n",
       "                                                                     grow_policy=None,\n",
       "                                                                     importance_type=None,\n",
       "                                                                     interaction_cons...\n",
       "                             'base_estimator__objective': Categorical(categories=('binary:logistic',), prior=None),\n",
       "                             'base_estimator__scale_pos_weight': Real(low=1e-06, high=10, prior='log-uniform', transform='normalize'),\n",
       "                             'base_estimator__subsample': Real(low=1e-06, high=1, prior='log-uniform', transform='normalize'),\n",
       "                             'base_estimator__tree_method': Categorical(categories=('exact', 'approx', 'hist'), prior=None)},\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesopt = BayesSearchCV(\n",
    "    ClassifierChain(xgb.XGBClassifier(), random_state=0),\n",
    "    {\n",
    "        \"base_estimator__objective\": Categorical([\"binary:logistic\"]),\n",
    "        \"base_estimator__max_depth\": Integer(1, 10),\n",
    "        \"base_estimator__min_child_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__max_delta_step\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__subsample\": Real(1e-6, 1, prior=\"log-uniform\"),\n",
    "        \"base_estimator__tree_method\": Categorical([\"exact\", \"approx\", \"hist\"]),\n",
    "        \"base_estimator__scale_pos_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__gamma\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__eta\": Real(1e-6, 1, prior=\"log-uniform\")\n",
    "    },\n",
    "    n_iter=250,\n",
    "    cv=5,\n",
    "    random_state=0,\n",
    "    n_jobs=1,\n",
    "    n_points=1,\n",
    "    scoring=make_scorer(multilabel_f1_wrapper),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "bayesopt.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 227\n",
      "Split scores:\n",
      " 0 0.820343579865027\n",
      " 1 0.8369469377938566\n",
      " 2 0.8299130604982062\n",
      " 3 0.8324157502908109\n",
      " 4 0.8283769438981426\n",
      "Mean score: 0.8295992544692087\n",
      "Best parameter combination found: OrderedDict([('base_estimator__eta', 0.01800129117578471), ('base_estimator__gamma', 0.024282066345515534), ('base_estimator__max_delta_step', 0.0037650147666433023), ('base_estimator__max_depth', 8), ('base_estimator__min_child_weight', 8.32868910859238), ('base_estimator__objective', 'binary:logistic'), ('base_estimator__scale_pos_weight', 3.084189256760568), ('base_estimator__subsample', 0.14890444759709648), ('base_estimator__tree_method', 'exact')])\n"
     ]
    }
   ],
   "source": [
    "best_iteration = 0\n",
    "for i in range(0, 250):\n",
    "    if bayesopt.cv_results_[\"mean_test_score\"][i] == bayesopt.best_score_:\n",
    "        best_iteration = i\n",
    "print(\"Best iteration:\", best_iteration)\n",
    "print(\"Split scores:\")\n",
    "for i in range(0, 5):\n",
    "    print(\"\", i, bayesopt.cv_results_[\"split\"+str(i)+\"_test_score\"][best_iteration])\n",
    "    \n",
    "print(\"Mean score:\", bayesopt.best_score_)\n",
    "print(\"Best parameter combination found:\", bayesopt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"xgb_s_aureus_raw.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_s_aureus_raw.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(bayesopt.best_estimator_, model_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main metrics:\n",
      " Hamming Loss: 0.13625648279113625\n",
      " Accuracy: 0.6605374823196606\n",
      " F1 Score (Weighted): 0.8323409200315045\n",
      "================================================\n",
      "Other metrics:\n",
      " F1 Score (Unweighted): 0.5495689021605566\n",
      " F1 Score (sklearn Unweighted): 0.17572362278244627\n",
      " F1 Score (sklearn Weighted): 0.24274180260339429\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_x)\n",
    "model_hl, model_acc, model_f1 = report(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(antibiotics), figsize=(len(antibiotics)*5, 5))\n",
    "fig.supxlabel(\"Predicted Label\")\n",
    "fig.supylabel(\"True Label\")\n",
    "\n",
    "cm_svm_c = multilabel_confusion_matrix(test_y, (pred > 0.5))\n",
    "\n",
    "for i in range(len(antibiotics)):\n",
    "  sns.heatmap(ax=axes[i], data=cm_svm_c[i], annot=True, fmt='d', cbar=None, cmap=\"Blues\", xticklabels=[\"S\", \"R\"], yticklabels=[\"S\", \"R\"]).set(title=antibiotics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(test_x)\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for antibiotic in range(len(antibiotics)):\n",
    "    count_tp = 0\n",
    "    count_tn = 0\n",
    "    count_fp = 0\n",
    "    count_fn = 0\n",
    "    sum_tp = 0\n",
    "    sum_tn = 0\n",
    "    sum_fp = 0\n",
    "    sum_fn = 0\n",
    "    for i in range(len(proba[:, antibiotic])):\n",
    "        disc_pred = int(proba[i, antibiotic] > 0.5)\n",
    "        if disc_pred == 1:\n",
    "            if disc_pred == test_y.iloc[i, antibiotic]:\n",
    "                count_tp += 1\n",
    "                sum_tp += proba[i, antibiotic]\n",
    "            else:\n",
    "                count_fp += 1\n",
    "                sum_fp += proba[i, antibiotic]\n",
    "        else:\n",
    "            if disc_pred == test_y.iloc[i, antibiotic]:\n",
    "                count_tn += 1\n",
    "                sum_tn += proba[i, antibiotic]\n",
    "            else:\n",
    "                count_fn += 1\n",
    "                sum_fn += proba[i, antibiotic]\n",
    "    print(\"Results for antibiotic\", antibiotics[antibiotic])\n",
    "    if count_tp == 0:\n",
    "        print(\" Mean TP: None\")\n",
    "    else: \n",
    "        print(\" Mean TP:\", sum_tp/count_tp)\n",
    "    if count_tn == 0:\n",
    "        print(\" Mean TN: None\")\n",
    "    else: \n",
    "        print(\" Mean TN:\", sum_tn/count_tn)\n",
    "    if count_fp == 0:\n",
    "        print(\" Mean FP: None\")\n",
    "    else: \n",
    "        print(\" Mean FP:\", sum_fp/count_fp)\n",
    "    if count_fn == 0:\n",
    "        print(\" Mean FN: None\")\n",
    "    else: \n",
    "        print(\" Mean FN:\", sum_fn/count_fn)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
