{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/castudil/bacteria-multi-label/blob/main/multilabel_bac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKUeIuZcpHUl"
   },
   "source": [
    "Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bzVprbfpWSLa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import (f1_score, multilabel_confusion_matrix,\n",
    "                             accuracy_score, hamming_loss, jaccard_score, make_scorer)\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from joblib import dump, load\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "nQsWnulDWdDD",
    "outputId": "6f97687c-b560-4e34-fd8b-9c3ef7aeb0c7",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.016147</td>\n",
       "      <td>0.016983</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.019405</td>\n",
       "      <td>0.023356</td>\n",
       "      <td>0.026224</td>\n",
       "      <td>0.026569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037966</td>\n",
       "      <td>0.030364</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>0.040851</td>\n",
       "      <td>0.034176</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.025638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009001</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014496</td>\n",
       "      <td>0.024966</td>\n",
       "      <td>0.027437</td>\n",
       "      <td>0.026541</td>\n",
       "      <td>0.022940</td>\n",
       "      <td>0.020572</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>0.020910</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.021436</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.020229</td>\n",
       "      <td>0.018818</td>\n",
       "      <td>0.018637</td>\n",
       "      <td>0.018815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024620</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.026715</td>\n",
       "      <td>0.032045</td>\n",
       "      <td>0.030431</td>\n",
       "      <td>0.029085</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.016407</td>\n",
       "      <td>0.018011</td>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.019116</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051312</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.049338</td>\n",
       "      <td>0.055039</td>\n",
       "      <td>0.054541</td>\n",
       "      <td>0.058643</td>\n",
       "      <td>0.058919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236769</td>\n",
       "      <td>0.217499</td>\n",
       "      <td>0.187244</td>\n",
       "      <td>0.216243</td>\n",
       "      <td>0.221910</td>\n",
       "      <td>0.226531</td>\n",
       "      <td>0.221965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>0.056616</td>\n",
       "      <td>0.039011</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>0.048517</td>\n",
       "      <td>0.050865</td>\n",
       "      <td>0.047771</td>\n",
       "      <td>0.049312</td>\n",
       "      <td>0.048257</td>\n",
       "      <td>0.049417</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021169</td>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.033694</td>\n",
       "      <td>0.021037</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>0.125837</td>\n",
       "      <td>0.107712</td>\n",
       "      <td>0.109186</td>\n",
       "      <td>0.107613</td>\n",
       "      <td>0.109855</td>\n",
       "      <td>0.105060</td>\n",
       "      <td>0.099640</td>\n",
       "      <td>0.104169</td>\n",
       "      <td>0.120303</td>\n",
       "      <td>0.125067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082375</td>\n",
       "      <td>0.083446</td>\n",
       "      <td>0.096510</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.092228</td>\n",
       "      <td>0.085599</td>\n",
       "      <td>0.042142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035603</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>0.042372</td>\n",
       "      <td>0.046666</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.043914</td>\n",
       "      <td>0.039875</td>\n",
       "      <td>0.037170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.006124</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049241</td>\n",
       "      <td>0.039586</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.039139</td>\n",
       "      <td>0.046816</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.022810</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>0.026694</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.025140</td>\n",
       "      <td>0.026609</td>\n",
       "      <td>0.024203</td>\n",
       "      <td>0.020953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090198</td>\n",
       "      <td>0.101889</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>0.122065</td>\n",
       "      <td>0.095740</td>\n",
       "      <td>0.082289</td>\n",
       "      <td>0.081714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2824 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2000      2001      2002      2003      2004      2005      2006   \n",
       "0     0.018721  0.016147  0.016983  0.021218  0.020846  0.019784  0.019405  \\\n",
       "1     0.009001  0.007475  0.006874  0.008575  0.009539  0.007894  0.008314   \n",
       "2     0.022354  0.020220  0.020910  0.024631  0.021436  0.021197  0.020229   \n",
       "3     0.017619  0.016073  0.016407  0.018011  0.019364  0.018950  0.017607   \n",
       "4     0.008264  0.008229  0.006753  0.006657  0.010107  0.007039  0.008250   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2819  0.056616  0.039011  0.040380  0.048517  0.050865  0.047771  0.049312   \n",
       "2820  0.125837  0.107712  0.109186  0.107613  0.109855  0.105060  0.099640   \n",
       "2821  0.000000  0.000000  0.035603  0.039994  0.042372  0.046666  0.045781   \n",
       "2822  0.005443  0.005998  0.003670  0.005588  0.006124  0.005019  0.004853   \n",
       "2823  0.026184  0.022810  0.023200  0.020464  0.026694  0.024624  0.025140   \n",
       "\n",
       "          2007      2008      2009  ...      9993      9994      9995   \n",
       "0     0.023356  0.026224  0.026569  ...  0.037966  0.030364  0.037545  \\\n",
       "1     0.008013  0.008664  0.008923  ...  0.014496  0.024966  0.027437   \n",
       "2     0.018818  0.018637  0.018815  ...  0.024620  0.022942  0.026715   \n",
       "3     0.019116  0.023623  0.024492  ...  0.051312  0.047458  0.049338   \n",
       "4     0.010670  0.008134  0.006513  ...  0.236769  0.217499  0.187244   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2819  0.048257  0.049417  0.049000  ...  0.021169  0.023617  0.033694   \n",
       "2820  0.104169  0.120303  0.125067  ...  0.082375  0.083446  0.096510   \n",
       "2821  0.043914  0.039875  0.037170  ...  0.006903  0.008322  0.011071   \n",
       "2822  0.005400  0.004169  0.005151  ...  0.049241  0.039586  0.050542   \n",
       "2823  0.026609  0.024203  0.020953  ...  0.090198  0.101889  0.105174   \n",
       "\n",
       "          9996      9997      9998      9999  Oxacillin  Clindamycin   \n",
       "0     0.040851  0.034176  0.046110  0.025638        0.0          0.0  \\\n",
       "1     0.026541  0.022940  0.020572  0.032504        0.0          0.0   \n",
       "2     0.032045  0.030431  0.029085  0.013117        0.0          0.0   \n",
       "3     0.055039  0.054541  0.058643  0.058919        0.0          0.0   \n",
       "4     0.216243  0.221910  0.226531  0.221965        0.0          0.0   \n",
       "...        ...       ...       ...       ...        ...          ...   \n",
       "2819  0.021037  0.018727  0.010641  0.009238        0.0          1.0   \n",
       "2820  0.084883  0.092228  0.085599  0.042142        0.0          1.0   \n",
       "2821  0.010274  0.004682  0.003547  0.001744        0.0          0.0   \n",
       "2822  0.039139  0.046816  0.043036  0.037402        1.0          1.0   \n",
       "2823  0.122065  0.095740  0.082289  0.081714        0.0          0.0   \n",
       "\n",
       "      Fusidic acid  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  \n",
       "...            ...  \n",
       "2819           0.0  \n",
       "2820           0.0  \n",
       "2821           0.0  \n",
       "2822           0.0  \n",
       "2823           0.0  \n",
       "\n",
       "[2824 rows x 8003 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = \"data/processed/raw/train_s_aureus_driams.csv\"\n",
    "train_bac = pd.read_csv(train_file)\n",
    "train_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>Oxacillin</th>\n",
       "      <th>Clindamycin</th>\n",
       "      <th>Fusidic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044453</td>\n",
       "      <td>0.032486</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>0.034223</td>\n",
       "      <td>0.037528</td>\n",
       "      <td>0.039503</td>\n",
       "      <td>0.031378</td>\n",
       "      <td>0.035506</td>\n",
       "      <td>0.037688</td>\n",
       "      <td>0.035658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247814</td>\n",
       "      <td>0.263833</td>\n",
       "      <td>0.279904</td>\n",
       "      <td>0.264432</td>\n",
       "      <td>0.241573</td>\n",
       "      <td>0.266020</td>\n",
       "      <td>0.231517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004318</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033364</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>0.066426</td>\n",
       "      <td>0.057485</td>\n",
       "      <td>0.052903</td>\n",
       "      <td>0.050839</td>\n",
       "      <td>0.036631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026184</td>\n",
       "      <td>0.026459</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.028609</td>\n",
       "      <td>0.031314</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.028051</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>0.028978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.094103</td>\n",
       "      <td>0.080969</td>\n",
       "      <td>0.073970</td>\n",
       "      <td>0.069047</td>\n",
       "      <td>0.070988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>0.014582</td>\n",
       "      <td>0.015084</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.018272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.007583</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060499</td>\n",
       "      <td>0.043034</td>\n",
       "      <td>0.030296</td>\n",
       "      <td>0.032635</td>\n",
       "      <td>0.031272</td>\n",
       "      <td>0.032762</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>0.030382</td>\n",
       "      <td>0.030233</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029452</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>0.039230</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>0.037219</td>\n",
       "      <td>0.022227</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.013092</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.009902</td>\n",
       "      <td>0.013513</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.010994</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050851</td>\n",
       "      <td>0.046334</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.066948</td>\n",
       "      <td>0.061007</td>\n",
       "      <td>0.056178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.021222</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>0.015512</td>\n",
       "      <td>0.017995</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.019427</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047285</td>\n",
       "      <td>0.056455</td>\n",
       "      <td>0.058002</td>\n",
       "      <td>0.055528</td>\n",
       "      <td>0.043539</td>\n",
       "      <td>0.039962</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>0.045613</td>\n",
       "      <td>0.041040</td>\n",
       "      <td>0.046177</td>\n",
       "      <td>0.050216</td>\n",
       "      <td>0.046525</td>\n",
       "      <td>0.048843</td>\n",
       "      <td>0.045920</td>\n",
       "      <td>0.044283</td>\n",
       "      <td>0.043983</td>\n",
       "      <td>0.046330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104234</td>\n",
       "      <td>0.094692</td>\n",
       "      <td>0.090012</td>\n",
       "      <td>0.096624</td>\n",
       "      <td>0.092228</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.043682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.015193</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>0.010474</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.014157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051772</td>\n",
       "      <td>0.058255</td>\n",
       "      <td>0.074609</td>\n",
       "      <td>0.068249</td>\n",
       "      <td>0.049157</td>\n",
       "      <td>0.070229</td>\n",
       "      <td>0.043615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.022209</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.025181</td>\n",
       "      <td>0.025069</td>\n",
       "      <td>0.025761</td>\n",
       "      <td>0.022690</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.022596</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052692</td>\n",
       "      <td>0.061853</td>\n",
       "      <td>0.048375</td>\n",
       "      <td>0.060665</td>\n",
       "      <td>0.043305</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>0.048284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>707 rows × 8003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2000      2001      2002      2003      2004      2005      2006   \n",
       "0    0.044453  0.032486  0.032540  0.034223  0.037528  0.039503  0.031378  \\\n",
       "1    0.004318  0.001881  0.001274  0.000902  0.000892  0.000049  0.002188   \n",
       "2    0.026184  0.026459  0.025393  0.028609  0.031314  0.031739  0.033337   \n",
       "3    0.000000  0.015010  0.017782  0.014582  0.015084  0.018046  0.014461   \n",
       "4    0.060499  0.043034  0.030296  0.032635  0.031272  0.032762  0.031154   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "702  0.013092  0.011730  0.009902  0.013513  0.015317  0.011370  0.013338   \n",
       "703  0.021222  0.015896  0.015512  0.017995  0.018663  0.019427  0.018667   \n",
       "704  0.045613  0.041040  0.046177  0.050216  0.046525  0.048843  0.045920   \n",
       "705  0.015193  0.011922  0.010877  0.009975  0.010474  0.012171  0.010417   \n",
       "706  0.025453  0.022209  0.022442  0.025181  0.025069  0.025761  0.022690   \n",
       "\n",
       "         2007      2008      2009  ...      9993      9994      9995   \n",
       "0    0.035506  0.037688  0.035658  ...  0.247814  0.263833  0.279904  \\\n",
       "1    0.002001  0.003081  0.003384  ...  0.033364  0.042735  0.066426   \n",
       "2    0.028051  0.028047  0.028978  ...  0.086746  0.087719  0.094103   \n",
       "3    0.014400  0.018769  0.018272  ...  0.005062  0.006748  0.004573   \n",
       "4    0.030382  0.030233  0.029438  ...  0.029452  0.033288  0.039230   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "702  0.010117  0.010994  0.009222  ...  0.050851  0.046334  0.042840   \n",
       "703  0.014589  0.016765  0.015071  ...  0.047285  0.056455  0.058002   \n",
       "704  0.044283  0.043983  0.046330  ...  0.104234  0.094692  0.090012   \n",
       "705  0.010783  0.013170  0.014157  ...  0.051772  0.058255  0.074609   \n",
       "706  0.024250  0.022596  0.025572  ...  0.052692  0.061853  0.048375   \n",
       "\n",
       "         9996      9997      9998      9999  Oxacillin  Clindamycin   \n",
       "0    0.264432  0.241573  0.266020  0.231517        0.0          0.0  \\\n",
       "1    0.057485  0.052903  0.050839  0.036631        0.0          0.0   \n",
       "2    0.080969  0.073970  0.069047  0.070988        0.0          1.0   \n",
       "3    0.007583  0.008427  0.004729  0.002394        0.0          0.0   \n",
       "4    0.046477  0.037219  0.022227  0.019920        0.0          0.0   \n",
       "..        ...       ...       ...       ...        ...          ...   \n",
       "702  0.061644  0.066948  0.061007  0.056178        0.0          0.0   \n",
       "703  0.055528  0.043539  0.039962  0.034761        1.0          0.0   \n",
       "704  0.096624  0.092228  0.100024  0.043682        0.0          0.0   \n",
       "705  0.068249  0.049157  0.070229  0.043615        1.0          0.0   \n",
       "706  0.060665  0.043305  0.041617  0.048284        0.0          0.0   \n",
       "\n",
       "     Fusidic acid  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             1.0  \n",
       "4             0.0  \n",
       "..            ...  \n",
       "702           0.0  \n",
       "703           0.0  \n",
       "704           1.0  \n",
       "705           0.0  \n",
       "706           0.0  \n",
       "\n",
       "[707 rows x 8003 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = \"data/processed/raw/test_s_aureus_driams.csv\"\n",
    "test_bac = pd.read_csv(test_file)\n",
    "test_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rXEshQwKQuzR",
    "outputId": "32756288-dd35-49f1-be9b-34bfe433baf7",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_bac[train_bac.columns.drop(list(train_bac.filter(regex='[^0-9]')))]\n",
    "test_x = test_bac[test_bac.columns.drop(list(test_bac.filter(regex='[^0-9]')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GiCCaGOEQuzS",
    "outputId": "bdc15429-4a19-4332-d59f-dd1c0ce37d88",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "antibiotics = train_bac.columns.drop(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_bac[antibiotics]\n",
    "test_y = test_bac[antibiotics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multilabel_f1_wrapper(true, pred, average=\"weighted\"):\n",
    "    if isinstance(true, list):\n",
    "        true = np.array(true)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        true = true.to_numpy()\n",
    "    if isinstance(pred, list):\n",
    "        pred = np.array(pred)\n",
    "    elif isinstance(true, pd.DataFrame):\n",
    "        pred = pred.to_numpy()\n",
    "    column = 0\n",
    "    total = 0\n",
    "    while column < true[0].size:\n",
    "        total+=f1_score(true[:, column], pred[:, column], average=average)\n",
    "        column+=1\n",
    "    return total/(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def report(true, pred):\n",
    "        \n",
    "    hl = hamming_loss(true, pred)\n",
    "    f1w = multilabel_f1_wrapper(true, pred, \"weighted\")\n",
    "    acc = accuracy_score(true, pred)\n",
    "    \n",
    "    f1u = multilabel_f1_wrapper(true, pred, \"macro\")\n",
    "    f1su = f1_score(true, pred, average=\"macro\")\n",
    "    f1sw = f1_score(true, pred, average=\"weighted\")\n",
    "\n",
    "    \n",
    "    print(\"Main metrics:\")\n",
    "    print(\" Hamming Loss:\", hl)\n",
    "    print(\" Accuracy:\", acc)\n",
    "    print(\" F1 Score (Weighted):\", f1w)\n",
    "    print(\"================================================\")\n",
    "    print(\"Other metrics:\")\n",
    "    print(\" F1 Score (Unweighted):\", f1u)\n",
    "    print(\" F1 Score (sklearn Unweighted):\", f1su)\n",
    "    print(\" F1 Score (sklearn Weighted):\", f1sw)\n",
    "    return hl, acc, f1w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-gP_gv8QuzZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=ClassifierChain(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                                     booster=None,\n",
       "                                                                     callbacks=None,\n",
       "                                                                     colsample_bylevel=None,\n",
       "                                                                     colsample_bynode=None,\n",
       "                                                                     colsample_bytree=None,\n",
       "                                                                     early_stopping_rounds=None,\n",
       "                                                                     enable_categorical=False,\n",
       "                                                                     eval_metric=None,\n",
       "                                                                     feature_types=None,\n",
       "                                                                     gamma=None,\n",
       "                                                                     gpu_id=None,\n",
       "                                                                     grow_policy=None,\n",
       "                                                                     importance_type=None,\n",
       "                                                                     interaction_cons...\n",
       "                             &#x27;base_estimator__objective&#x27;: Categorical(categories=(&#x27;binary:logistic&#x27;,), prior=None),\n",
       "                             &#x27;base_estimator__scale_pos_weight&#x27;: Real(low=1e-06, high=10, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__subsample&#x27;: Real(low=1e-06, high=1, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__tree_method&#x27;: Categorical(categories=(&#x27;exact&#x27;, &#x27;approx&#x27;, &#x27;hist&#x27;), prior=None)},\n",
       "              verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=ClassifierChain(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                                     booster=None,\n",
       "                                                                     callbacks=None,\n",
       "                                                                     colsample_bylevel=None,\n",
       "                                                                     colsample_bynode=None,\n",
       "                                                                     colsample_bytree=None,\n",
       "                                                                     early_stopping_rounds=None,\n",
       "                                                                     enable_categorical=False,\n",
       "                                                                     eval_metric=None,\n",
       "                                                                     feature_types=None,\n",
       "                                                                     gamma=None,\n",
       "                                                                     gpu_id=None,\n",
       "                                                                     grow_policy=None,\n",
       "                                                                     importance_type=None,\n",
       "                                                                     interaction_cons...\n",
       "                             &#x27;base_estimator__objective&#x27;: Categorical(categories=(&#x27;binary:logistic&#x27;,), prior=None),\n",
       "                             &#x27;base_estimator__scale_pos_weight&#x27;: Real(low=1e-06, high=10, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__subsample&#x27;: Real(low=1e-06, high=1, prior=&#x27;log-uniform&#x27;, transform=&#x27;normalize&#x27;),\n",
       "                             &#x27;base_estimator__tree_method&#x27;: Categorical(categories=(&#x27;exact&#x27;, &#x27;approx&#x27;, &#x27;hist&#x27;), prior=None)},\n",
       "              verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(base_estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                             callbacks=None,\n",
       "                                             colsample_bylevel=None,\n",
       "                                             colsample_bynode=None,\n",
       "                                             colsample_bytree=None,\n",
       "                                             early_stopping_rounds=None,\n",
       "                                             enable_categorical=False,\n",
       "                                             eval_metric=None,\n",
       "                                             feature_types=None, gamma=None,\n",
       "                                             gpu_id=None, grow_policy=None,\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate=None, max_bin=None,\n",
       "                                             max_cat_threshold=None,\n",
       "                                             max_cat_to_onehot=None,\n",
       "                                             max_delta_step=None,\n",
       "                                             max_depth=None, max_leaves=None,\n",
       "                                             min_child_weight=None, missing=nan,\n",
       "                                             monotone_constraints=None,\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             num_parallel_tree=None,\n",
       "                                             predictor=None, random_state=None, ...),\n",
       "                random_state=0)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=ClassifierChain(base_estimator=XGBClassifier(base_score=None,\n",
       "                                                                     booster=None,\n",
       "                                                                     callbacks=None,\n",
       "                                                                     colsample_bylevel=None,\n",
       "                                                                     colsample_bynode=None,\n",
       "                                                                     colsample_bytree=None,\n",
       "                                                                     early_stopping_rounds=None,\n",
       "                                                                     enable_categorical=False,\n",
       "                                                                     eval_metric=None,\n",
       "                                                                     feature_types=None,\n",
       "                                                                     gamma=None,\n",
       "                                                                     gpu_id=None,\n",
       "                                                                     grow_policy=None,\n",
       "                                                                     importance_type=None,\n",
       "                                                                     interaction_cons...\n",
       "                             'base_estimator__objective': Categorical(categories=('binary:logistic',), prior=None),\n",
       "                             'base_estimator__scale_pos_weight': Real(low=1e-06, high=10, prior='log-uniform', transform='normalize'),\n",
       "                             'base_estimator__subsample': Real(low=1e-06, high=1, prior='log-uniform', transform='normalize'),\n",
       "                             'base_estimator__tree_method': Categorical(categories=('exact', 'approx', 'hist'), prior=None)},\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesopt = BayesSearchCV(\n",
    "    ClassifierChain(xgb.XGBClassifier(), random_state=0),\n",
    "    {\n",
    "        \"base_estimator__objective\": Categorical([\"binary:logistic\"]),\n",
    "        \"base_estimator__max_depth\": Integer(1, 10),\n",
    "        \"base_estimator__min_child_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__max_delta_step\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__subsample\": Real(1e-6, 1, prior=\"log-uniform\"),\n",
    "        \"base_estimator__tree_method\": Categorical([\"exact\", \"approx\", \"hist\"]),\n",
    "        \"base_estimator__scale_pos_weight\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__gamma\": Real(1e-6, 10, prior=\"log-uniform\"),\n",
    "        \"base_estimator__eta\": Real(1e-6, 1, prior=\"log-uniform\")\n",
    "    },\n",
    "    n_iter=250,\n",
    "    cv=5,\n",
    "    random_state=0,\n",
    "    n_jobs=1,\n",
    "    n_points=1,\n",
    "    scoring=make_scorer(multilabel_f1_wrapper),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "bayesopt.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 227\n",
      "Split scores:\n",
      " 0 0.820343579865027\n",
      " 1 0.8369469377938566\n",
      " 2 0.8299130604982062\n",
      " 3 0.8324157502908109\n",
      " 4 0.8283769438981426\n",
      "Mean score: 0.8295992544692087\n",
      "Best parameter combination found: OrderedDict([('base_estimator__eta', 0.01800129117578471), ('base_estimator__gamma', 0.024282066345515534), ('base_estimator__max_delta_step', 0.0037650147666433023), ('base_estimator__max_depth', 8), ('base_estimator__min_child_weight', 8.32868910859238), ('base_estimator__objective', 'binary:logistic'), ('base_estimator__scale_pos_weight', 3.084189256760568), ('base_estimator__subsample', 0.14890444759709648), ('base_estimator__tree_method', 'exact')])\n"
     ]
    }
   ],
   "source": [
    "best_iteration = 0\n",
    "for i in range(0, 250):\n",
    "    if bayesopt.cv_results_[\"mean_test_score\"][i] == bayesopt.best_score_:\n",
    "        best_iteration = i\n",
    "print(\"Best iteration:\", best_iteration)\n",
    "print(\"Split scores:\")\n",
    "for i in range(0, 5):\n",
    "    print(\"\", i, bayesopt.cv_results_[\"split\"+str(i)+\"_test_score\"][best_iteration])\n",
    "    \n",
    "print(\"Mean score:\", bayesopt.best_score_)\n",
    "print(\"Best parameter combination found:\", bayesopt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"xgb_s_aureus_raw.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_s_aureus_raw.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(bayesopt.best_estimator_, model_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main metrics:\n",
      " Hamming Loss: 0.13625648279113625\n",
      " Accuracy: 0.6605374823196606\n",
      " F1 Score (Weighted): 0.8323409200315045\n",
      "================================================\n",
      "Other metrics:\n",
      " F1 Score (Unweighted): 0.5495689021605566\n",
      " F1 Score (sklearn Unweighted): 0.17572362278244627\n",
      " F1 Score (sklearn Weighted): 0.24274180260339429\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y) \n",
    "pred = model.predict(test_x)\n",
    "model_hl, model_acc, model_f1 = report(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, len(antibiotics), figsize=(len(antibiotics)*5, 5))\n",
    "fig.supxlabel(\"Predicted Label\")\n",
    "fig.supylabel(\"True Label\")\n",
    "\n",
    "cm_svm_c = multilabel_confusion_matrix(test_y, (pred > 0.5))\n",
    "\n",
    "for i in range(len(antibiotics)):\n",
    "  sns.heatmap(ax=axes[i], data=cm_svm_c[i], annot=True, fmt='d', cbar=None, cmap=\"Blues\", xticklabels=[\"S\", \"R\"], yticklabels=[\"S\", \"R\"]).set(title=antibiotics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49925449, 0.49935615, 0.49833953],\n",
       "       [0.49905112, 0.49925449, 0.49844122],\n",
       "       [0.49864447, 0.4984751 , 0.49830559],\n",
       "       ...,\n",
       "       [0.4987123 , 0.49844122, 0.4984751 ],\n",
       "       [0.49955943, 0.49840733, 0.49833953],\n",
       "       [0.49837339, 0.49830559, 0.49830559]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = model.predict_proba(test_x)\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for antibiotic Oxacillin\n",
      " Mean TP: 0.5003893181335094\n",
      " Mean TN: 0.49906753483745786\n",
      " Mean FP: 0.5002792286872864\n",
      " Mean FN: 0.4994136465318275\n",
      "Results for antibiotic Clindamycin\n",
      " Mean TP: 0.5004193633794785\n",
      " Mean TN: 0.4987343462225893\n",
      " Mean FP: 0.500277613218014\n",
      " Mean FN: 0.4987566220633527\n",
      "Results for antibiotic Fusidic acid\n",
      " Mean TP: None\n",
      " Mean TN: 0.49842448235458475\n",
      " Mean FP: None\n",
      " Mean FN: 0.4983929793039958\n"
     ]
    }
   ],
   "source": [
    "for antibiotic in range(len(antibiotics)):\n",
    "    count_tp = 0\n",
    "    count_tn = 0\n",
    "    count_fp = 0\n",
    "    count_fn = 0\n",
    "    sum_tp = 0\n",
    "    sum_tn = 0\n",
    "    sum_fp = 0\n",
    "    sum_fn = 0\n",
    "    for i in range(len(proba[:, antibiotic])):\n",
    "        disc_pred = int(proba[i, antibiotic] > 0.5)\n",
    "        if disc_pred == 1:\n",
    "            if disc_pred == test_y.iloc[i, antibiotic]:\n",
    "                count_tp += 1\n",
    "                sum_tp += proba[i, antibiotic]\n",
    "            else:\n",
    "                count_fp += 1\n",
    "                sum_fp += proba[i, antibiotic]\n",
    "        else:\n",
    "            if disc_pred == test_y.iloc[i, antibiotic]:\n",
    "                count_tn += 1\n",
    "                sum_tn += proba[i, antibiotic]\n",
    "            else:\n",
    "                count_fn += 1\n",
    "                sum_fn += proba[i, antibiotic]\n",
    "    print(\"Results for antibiotic\", antibiotics[antibiotic])\n",
    "    if count_tp == 0:\n",
    "        print(\" Mean TP: None\")\n",
    "    else: \n",
    "        print(\" Mean TP:\", sum_tp/count_tp)\n",
    "    if count_tn == 0:\n",
    "        print(\" Mean TN: None\")\n",
    "    else: \n",
    "        print(\" Mean TN:\", sum_tn/count_tn)\n",
    "    if count_fp == 0:\n",
    "        print(\" Mean FP: None\")\n",
    "    else: \n",
    "        print(\" Mean FP:\", sum_fp/count_fp)\n",
    "    if count_fn == 0:\n",
    "        print(\" Mean FN: None\")\n",
    "    else: \n",
    "        print(\" Mean FN:\", sum_fn/count_fn)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
